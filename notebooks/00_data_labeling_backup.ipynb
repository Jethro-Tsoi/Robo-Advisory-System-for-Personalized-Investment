{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Requirement already satisfied: polars in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (1.26.0)\nRequirement already satisfied: requests in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (2.32.3)\nRequirement already satisfied: tqdm in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (4.67.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests) (2025.1.31)\n"}], "source": "!pip install polars requests tqdm"}, {"cell_type": "code", "execution_count": 2, "id": "7d595140", "metadata": {}, "outputs": [], "source": "import os\nimport polars as pl\nimport requests\nimport json\nfrom tqdm import tqdm\nimport time\nimport re\n# Removed: # Removed: import google.generativeai as genai\nfrom concurrent.futures import ThreadPoolExecutor"}, {"cell_type": "code", "execution_count": 3, "id": "e855cc1e", "metadata": {}, "outputs": [], "source": "class KeyManager:\n    \"\"\"Manages multiple API keys and rotates them when rate limits are reached\"\"\"\n    \n    def __init__(self, env_prefix='MISTRAL_API_KEY'):\n        \"\"\"\n        Initialize the key manager\n        \n        Parameters:\n        -----------\n        env_prefix : str\n            Prefix for environment variables storing API keys.\n            Keys should be named like MISTRAL_API_KEY, MISTRAL_API_KEY_1, MISTRAL_API_KEY_2, etc.\n        \"\"\"\n        self.env_prefix = env_prefix\n        self.api_keys = self._load_api_keys()\n        self.current_index = 0\n        self.rate_limited_keys = {}  # Track which keys hit rate limits and when they can be used again\n        self.base_url = \"https://api.mistral.ai/v1/chat/completions\"\n        self.headers = {\n            \"Content-Type\": \"application/json\"\n        }\n        \n        if not self.api_keys:\n            print(\"\u26a0\ufe0f No API keys found. Please set at least one API key.\")\n            key = input(f\"Enter your Mistral AI API key: \")\n            if key:\n                self.api_keys.append(key)\n            else:\n                raise ValueError(\"No API key provided. Cannot continue.\")\n        \n        print(f\"Loaded {len(self.api_keys)} Mistral API keys.\")\n        \n    def _load_api_keys(self):\n        \"\"\"Load API keys from environment variables\"\"\"\n        api_keys = []\n        \n        # Try the base key first\n        base_key = os.getenv(self.env_prefix)\n        if base_key:\n            api_keys.append(base_key)\n        \n        # Try numbered keys (MISTRAL_API_KEY_1, MISTRAL_API_KEY_2, etc.)\n        for i in range(1, 10):  # Check for up to 10 keys\n            key = os.getenv(f\"{self.env_prefix}_{i}\")\n            if key:\n                api_keys.append(key)\n        \n        return api_keys\n    \n    def get_current_key(self):\n        \"\"\"Get the current active API key\"\"\"\n        return self.api_keys[self.current_index]\n    \n    def get_current_headers(self):\n        \"\"\"Get headers with the current API key\"\"\"\n        headers = self.headers.copy()\n        headers[\"Authorization\"] = f\"Bearer {self.get_current_key()}\"\n        return headers\n    \n    def rotate_key(self, rate_limited=False, retry_after=60):\n        \"\"\"\n        Rotate to the next available API key\n        \n        Parameters:\n        -----------\n        rate_limited : bool\n            Whether the current key hit a rate limit\n        retry_after : int\n            Seconds until the rate-limited key can be used again\n        \n        Returns:\n        --------\n        str : The new API key\n        \"\"\"\n        # Mark the current key as rate limited if needed\n        if rate_limited:\n            self.rate_limited_keys[self.current_index] = time.time() + retry_after\n            print(f\"API key {self.current_index + 1} rate limited. Will retry after {retry_after} seconds.\")\n        \n        # Try to find a key that's not rate limited\n        original_index = self.current_index\n        while True:\n            self.current_index = (self.current_index + 1) % len(self.api_keys)\n            \n            # Check if this key is rate limited\n            if self.current_index in self.rate_limited_keys:\n                # Check if enough time has passed\n                if time.time() > self.rate_limited_keys[self.current_index]:\n                    # Key is no longer rate limited\n                    del self.rate_limited_keys[self.current_index]\n                    break\n            else:\n                # Key is not rate limited\n                break\n                \n            # If we've checked all keys and they're all rate limited, use the least recently rate limited one\n            if self.current_index == original_index:\n                # Find the key that will be available soonest\n                soonest_available = min(self.rate_limited_keys.items(), key=lambda x: x[1])\n                self.current_index = soonest_available[0]\n                wait_time = max(0, self.rate_limited_keys[self.current_index] - time.time())\n                \n                if wait_time > 0:\n                    print(f\"All API keys are rate limited. Waiting {wait_time:.1f} seconds for the next available key.\")\n                    time.sleep(wait_time)\n                    del self.rate_limited_keys[self.current_index]\n                break\n        \n        # Return the new key\n        key = self.api_keys[self.current_index]\n        print(f\"Switched to Mistral API key {self.current_index + 1}\")\n        return key"}, {"cell_type": "code", "execution_count": 4, "id": "8e42b84c", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Loaded 1 Mistral API keys.\n"}], "source": "# Initialize the Key Manager\nkey_manager = KeyManager()\n\n# Set the model name\nMODEL = \"mistral-large-latest\"  # Using Mistral\\'s large model  # Using Mistral's large model instead of Gemini\n"}, {"cell_type": "markdown", "id": "c2c9bc61", "metadata": {}, "source": "\n> **\u26a0\ufe0f API Key Setup**\n>\n> This notebook uses the Mistral AI API with support for multiple API keys:\n>\n> 1. Set your primary API key as `MISTRAL_API_KEY` environment variable\n> 2. For additional keys, use `MISTRAL_API_KEY_1`, `MISTRAL_API_KEY_2`, etc.\n> 3. The system will automatically rotate between keys if rate limits are encountered\n>\n> Keys can be created at [Mistral AI Platform](https://console.mistral.ai/)\n>\n> **Mistral AI Free Tier Limits:**\n> - 1 request per second (60 requests per minute)\n> - 500,000 tokens per minute\n> - 1 billion tokens per month\n"}, {"cell_type": "code", "execution_count": 5, "id": "700448bb", "metadata": {}, "outputs": [], "source": "def setup_prompt(text):\n    \"\"\"Configure the prompt for Mistral\"\"\"\n    return [\n        {\"role\": \"system\", \"content\": \"\"\"\n            You are a financial sentiment analyzer. Classify the given tweet's sentiment into one of these categories:\n\n            STRONGLY_POSITIVE - Very bullish, highly confident optimistic outlook\n            POSITIVE - Generally optimistic, bullish view\n            NEUTRAL - Factual, balanced, or no clear sentiment\n            NEGATIVE - Generally pessimistic, bearish view\n            STRONGLY_NEGATIVE - Very bearish, highly confident pessimistic outlook\n\n            Examples:\n            \"Breaking: Company XYZ doubles profit forecast!\" -> STRONGLY_POSITIVE\n            \"Expecting modest gains next quarter\" -> POSITIVE\n            \"Market closed at 35,000\" -> NEUTRAL\n            \"Concerned about rising rates\" -> NEGATIVE\n            \"Crash incoming, sell everything!\" -> STRONGLY_NEGATIVE\n\n            Format: Return only one word from: STRONGLY_POSITIVE, POSITIVE, NEUTRAL, NEGATIVE, STRONGLY_NEGATIVE\n        \"\"\"},\n        {\"role\": \"user\", \"content\": f\"Analyze the sentiment of this tweet: {text}\"}\n    ]"}, {"cell_type": "code", "execution_count": null, "id": "fbf82b2f", "metadata": {}, "outputs": [], "source": "def get_sentiment(text, retries=3):\n    \"\"\"Get sentiment from Mistral with retry logic\"\"\"\n    if not text or len(str(text).strip()) < 3:\n        return 'NEUTRAL'\n    \n    for attempt in range(retries):\n        try:\n            # Prepare the API request\n            prompt = setup_prompt(text)\n            headers = key_manager.get_current_headers()\n            \n            payload = {\n                \"model\": MODEL,\n                \"messages\": prompt,\n                \"temperature\": 0.0,  # Deterministic output\n                \"max_tokens\": 5      # We only need one word\n            }\n            \n            # Call the Mistral API\n            response = requests.post(\n                key_manager.base_url,\n                headers=headers,\n                json=payload\n            )\n            \n            # Check for successful response\n            if response.status_code == 200:\n                result = response.json()\n                if \"choices\" in result and len(result[\"choices\"]) > 0:\n                    sentiment = result[\"choices\"][0][\"message\"][\"content\"].strip().upper()\n                    \n                    # Validate the response\n                    valid_labels = [\n                        'STRONGLY_POSITIVE', 'POSITIVE', 'NEUTRAL', 'NEGATIVE', 'STRONGLY_NEGATIVE'\n                    ]\n                    \n                    if sentiment in valid_labels:\n                        return sentiment\n                    else:\n                        print(f\"Invalid sentiment received: {sentiment}, defaulting to NEUTRAL\")\n                        return 'NEUTRAL'\n            \n            # Handle rate limiting\n            elif response.status_code == 429:\n                retry_after = 60  # Default retry time\n                if \"retry-after\" in response.headers:\n                    retry_after = int(response.headers[\"retry-after\"])\n                \n                # Switch to another API key if there are multiple keys\n                if len(key_manager.api_keys) > 1:\n                    key_manager.rotate_key(rate_limited=True, retry_after=retry_after)\n                    if attempt < retries - 1:\n                        continue\n                else:\n                    # Only one key, just wait\n                    wait_time = min(2 ** attempt * 5, retry_after)  # Exponential backoff with max retry_after\n                    print(f\"Rate limit hit - waiting {wait_time}s before retry ({attempt+1}/{retries})\")\n                    time.sleep(wait_time)\n                    if attempt < retries - 1:\n                        continue\n            \n            # Handle other errors\n            else:\n                print(f\"API error {response.status_code}: {response.text}\")\n                if attempt < retries - 1:\n                    time.sleep(2)  # Wait before retry\n                    continue\n        \n        except Exception as e:\n            if attempt == retries - 1:\n                print(f\"Error processing text: {str(text)[:50]}...\\nError: {str(e)}\")\n                return 'NEUTRAL'\n            time.sleep(2)  # Wait before retry\n    \n    return 'NEUTRAL'\n"}, {"cell_type": "code", "execution_count": 7, "id": "7cc0b913", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Invalid sentiment received: STRONGLY_POS, defaulting to NEUTRAL\nTest tweet: 'Breaking: Tesla stock hits all-time high after unexpected profit surge'\nSentiment: NEUTRAL\nUsing Mistral API key index: 0\n"}], "source": "# Test the sentiment analysis with key rotation\ntest_tweet = \"Breaking: Tesla stock hits all-time high after unexpected profit surge\"\nsentiment = get_sentiment(test_tweet)\nprint(f\"Test tweet: '{test_tweet}'\")\nprint(f\"Sentiment: {sentiment}\")\nprint(f\"Using Mistral API key index: {key_manager.current_index}\")\n"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Loading stock market tweets dataset from Hugging Face...\nLoaded 1700641 tweets\n\nSample tweets:\n"}, {"data": {"text/html": "<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th></th><th>tweet_id</th><th>writer</th><th>post_date</th><th>body</th><th>comment_num</th><th>retweet_num</th><th>like_num</th><th>ticker_symbol</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>550441672312512512</td><td>&quot;KeralaGuy77&quot;</td><td>&quot;2015-01-01&quot;</td><td>&quot;Insanity of today weirdo massi\u2026</td><td>0</td><td>0</td><td>0</td><td>&quot;AAPL&quot;</td></tr><tr><td>1</td><td>550452877466935296</td><td>&quot;TheTrendIsUp&quot;</td><td>&quot;2015-01-01&quot;</td><td>&quot;My biggest winner in 2014: Inv\u2026</td><td>1</td><td>0</td><td>0</td><td>&quot;AAPL&quot;</td></tr><tr><td>2</td><td>550456665607122944</td><td>&quot;t_nathan95&quot;</td><td>&quot;2015-01-01&quot;</td><td>&quot;Had a down day of -.66%. Worst\u2026</td><td>0</td><td>0</td><td>0</td><td>&quot;AAPL&quot;</td></tr><tr><td>3</td><td>550459042787651584</td><td>&quot;petergo99037185&quot;</td><td>&quot;2015-01-01&quot;</td><td>&quot;YR %,&nbsp;&nbsp;/-, $TSLA&nbsp;&nbsp;47.85%, $FB \u2026</td><td>0</td><td>0</td><td>0</td><td>&quot;AAPL&quot;</td></tr><tr><td>4</td><td>550461555423584257</td><td>&quot;t_nathan95&quot;</td><td>&quot;2015-01-01&quot;</td><td>&quot;Prediction: $TWTR $GRPN $YELP \u2026</td><td>0</td><td>0</td><td>1</td><td>&quot;GOOG&quot;</td></tr></tbody></table></div>", "text/plain": "shape: (5, 9)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     \u2506 tweet_id   \u2506 writer     \u2506 post_date  \u2506 \u2026 \u2506 comment_nu \u2506 retweet_nu \u2506 like_num \u2506 ticker_sym \u2502\n\u2502 --- \u2506 ---        \u2506 ---        \u2506 ---        \u2506   \u2506 m          \u2506 m          \u2506 ---      \u2506 bol        \u2502\n\u2502 i64 \u2506 i64        \u2506 str        \u2506 str        \u2506   \u2506 ---        \u2506 ---        \u2506 i64      \u2506 ---        \u2502\n\u2502     \u2506            \u2506            \u2506            \u2506   \u2506 i64        \u2506 i64        \u2506          \u2506 str        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 5504416723 \u2506 KeralaGuy7 \u2506 2015-01-01 \u2506 \u2026 \u2506 0          \u2506 0          \u2506 0        \u2506 AAPL       \u2502\n\u2502     \u2506 12512512   \u2506 7          \u2506            \u2506   \u2506            \u2506            \u2506          \u2506            \u2502\n\u2502 1   \u2506 5504528774 \u2506 TheTrendIs \u2506 2015-01-01 \u2506 \u2026 \u2506 1          \u2506 0          \u2506 0        \u2506 AAPL       \u2502\n\u2502     \u2506 66935296   \u2506 Up         \u2506            \u2506   \u2506            \u2506            \u2506          \u2506            \u2502\n\u2502 2   \u2506 5504566656 \u2506 t_nathan95 \u2506 2015-01-01 \u2506 \u2026 \u2506 0          \u2506 0          \u2506 0        \u2506 AAPL       \u2502\n\u2502     \u2506 07122944   \u2506            \u2506            \u2506   \u2506            \u2506            \u2506          \u2506            \u2502\n\u2502 3   \u2506 5504590427 \u2506 petergo990 \u2506 2015-01-01 \u2506 \u2026 \u2506 0          \u2506 0          \u2506 0        \u2506 AAPL       \u2502\n\u2502     \u2506 87651584   \u2506 37185      \u2506            \u2506   \u2506            \u2506            \u2506          \u2506            \u2502\n\u2502 4   \u2506 5504615554 \u2506 t_nathan95 \u2506 2015-01-01 \u2506 \u2026 \u2506 0          \u2506 0          \u2506 1        \u2506 GOOG       \u2502\n\u2502     \u2506 23584257   \u2506            \u2506            \u2506   \u2506            \u2506            \u2506          \u2506            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "# Load data from Hugging Face\nprint(\"Loading stock market tweets dataset from Hugging Face...\")\n\n# Check if the huggingface datasets library is installed\ntry:\n    import huggingface_hub\nexcept ImportError:\n    print(\"Installing huggingface_hub...\")\n    !pip install huggingface_hub\n    import huggingface_hub\n\n# Load the dataset using Polars\ndf = pl.read_csv('hf://datasets/mjw/stock_market_tweets/stock_market_tweets.csv')\n\nprint(f\"Loaded {df.shape[0]} tweets\")\nprint(\"\\nSample tweets:\")\ndf.head(5)"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Using 'body' column for tweet text\n\nAnalyzing sentiment for 1000 tweets\n"}], "source": "# Prepare the dataset for sentiment analysis\n# Let's make sure we have the 'body' column which contains the tweet text\nif 'body' in df.columns:\n    tweet_column = 'body'\nelif 'full_text' in df.columns:\n    tweet_column = 'full_text'\nelse:\n    raise ValueError(\"Could not find tweet text column in the dataset\")\n\nprint(f\"Using '{tweet_column}' column for tweet text\")\n\n# For demonstration, let's use a small subset of the data\n# Use all data instead of a sample - WARNING: This will process 1.7M tweets!\nsample_size = df.shape[0]  # Adjust based on your needs\nsample_df = df.sample(sample_size, seed=42)\n\nprint(f\"\\nAnalyzing sentiment for {sample_size} tweets\")"}, {"cell_type": "markdown", "id": "2ecb1756", "metadata": {}, "source": "\n## Rate Limits for Mistral AI API\n\nMistral AI's free tier has the following limits:\n- 1 request per second (60 requests per minute)\n- 500,000 tokens per minute\n- 1 billion tokens per month\n\nThis notebook implements:\n1. Key rotation to handle multiple API keys\n2. Automatic retry with exponential backoff\n3. Batch processing to optimize throughput\n4. Error handling to ensure robust processing\n\nIf you need to process many tweets, consider:\n- Creating multiple API keys\n- Adjusting batch size and workers based on your needs\n- Processing tweets in smaller batches with appropriate delays\n"}, {"cell_type": "code", "execution_count": null, "id": "c0a0cf98", "metadata": {}, "outputs": [], "source": "def process_tweets(tweets, batch_size=4, max_workers=2):\n    \"\"\"Process tweets in batches with Google Gemini API\n    \n    Mistral AI free tier allows:\n    - Up to 1,500 requests per day\n    - Higher throughput than OpenRouter's free tier\n    \"\"\"\n    all_sentiments = []\n    \n    # For demonstration, we'll process a reasonable number of tweets\n    # Adjust max_tweets if needed - 200 is safe for the free tier\n    # Process all tweets without limitation\n    # Process all tweets\n    \n    print(f\"Processing {len(tweets)} tweets using Mistral AI API with {len(key_manager.api_keys)} API keys\")\n    \n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        for i in tqdm(range(0, len(tweets), batch_size), desc=\"Processing tweet batches\"):\n            batch = tweets[i:i+batch_size]\n            \n            try:\n                # Process tweets in parallel\n                results = list(executor.map(get_sentiment, batch))\n                all_sentiments.extend(results)\n                \n                # Add a delay between batches to respect Mistral\\'s 1 req/sec rate limit\n                # With batch_size and max_workers, adjust sleep time accordingly\n                time.sleep(batch_size * max_workers)  # Conservative approach\n                \n            except Exception as e:\n                print(f\"Error processing batch: {e}\")\n                # Add neutral sentiments for this batch in case of failure\n                all_sentiments.extend(['NEUTRAL'] * len(batch))\n                time.sleep(5)  # Wait a bit longer after an error\n                \n    # If we didn't get enough sentiments (due to errors), fill with NEUTRAL\n    if len(all_sentiments) < len(tweets):\n        all_sentiments.extend(['NEUTRAL'] * (len(tweets) - len(all_sentiments)))\n            \n    return all_sentiments"}, {"cell_type": "code", "execution_count": null, "id": "48ef2b4f", "metadata": {}, "outputs": [], "source": "# Test the sentiment analysis with key rotation\ntest_tweet = \"Breaking: Tesla stock hits all-time high after unexpected profit surge\"\nsentiment = get_sentiment(test_tweet)\nprint(f\"Test tweet: '{test_tweet}'\")\nprint(f\"Sentiment: {sentiment}\")\nprint(f\"Using Mistral API key index: {key_manager.current_index}\")\n"}, {"cell_type": "code", "execution_count": 10, "id": "b23691ac", "metadata": {}, "outputs": [], "source": "def process_tweets(tweets, batch_size=4, max_workers=2):\n    \"\"\"Process tweets in batches with Mistral AI API\n    \n    Mistral AI free tier allows:\n    - 1 request per second (60 requests per minute)\n    - 500,000 tokens per minute\n    - 1 billion tokens per month\n    \"\"\"\n    all_sentiments = []\n    \n    # Process all tweets\n    # Removed limitation: # Process all tweets without limitation\n    # Removed limitation: # Process all tweets\n    \n    print(f\"Processing {len(tweets)} tweets using Mistral AI API with {len(key_manager.api_keys)} API keys\")\n    \n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        for i in tqdm(range(0, len(tweets), batch_size), desc=\"Processing tweet batches\"):\n            batch = tweets[i:i+batch_size]\n            \n            try:\n                # Process tweets in parallel\n                results = list(executor.map(get_sentiment, batch))\n                all_sentiments.extend(results)\n                \n                # Add a delay between batches to respect Mistral's 1 req/sec rate limit\n                # With batch_size and max_workers, adjust sleep time accordingly\n                time.sleep(batch_size * max_workers)  # Conservative approach\n                \n            except Exception as e:\n                print(f\"Error processing batch: {e}\")\n                # Add neutral sentiments for this batch in case of failure\n                all_sentiments.extend(['NEUTRAL'] * len(batch))\n                time.sleep(5)  # Wait a bit longer after an error\n                \n    # If we didn't get enough sentiments (due to errors), fill with NEUTRAL\n    if len(all_sentiments) < len(tweets):\n        all_sentiments.extend(['NEUTRAL'] * (len(tweets) - len(all_sentiments)))\n            \n    return all_sentiments"}, {"cell_type": "code", "execution_count": 11, "id": "10544ce6", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Processing 200 tweets using Mistral AI API with 1 API keys\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:   0%|          | 0/50 [00:00<?, ?it/s]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:   2%|\u258f         | 1/50 [00:06<05:09,  6.32s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:   4%|\u258d         | 2/50 [00:17<07:18,  9.15s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:   6%|\u258c         | 3/50 [00:23<06:11,  7.90s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 10s before retry (2/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:   8%|\u258a         | 4/50 [00:40<08:40, 11.32s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  10%|\u2588         | 5/50 [00:46<07:10,  9.56s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  12%|\u2588\u258f        | 6/50 [00:53<06:09,  8.40s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 10s before retry (2/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  14%|\u2588\u258d        | 7/50 [01:09<08:00, 11.18s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nInvalid sentiment received: STRONGLY_N, defaulting to NEUTRAL\nInvalid sentiment received: STRONGLY_N, defaulting to NEUTRAL\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  16%|\u2588\u258c        | 8/50 [01:16<06:43,  9.60s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  18%|\u2588\u258a        | 9/50 [01:22<05:56,  8.69s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  20%|\u2588\u2588        | 10/50 [01:29<05:17,  7.94s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Invalid sentiment received: STRONGLY_POS, defaulting to NEUTRAL\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  22%|\u2588\u2588\u258f       | 11/50 [01:35<04:52,  7.49s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  24%|\u2588\u2588\u258d       | 12/50 [01:42<04:32,  7.18s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 10s before retry (2/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  26%|\u2588\u2588\u258c       | 13/50 [01:58<06:12, 10.08s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  30%|\u2588\u2588\u2588       | 15/50 [02:07<03:56,  6.76s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nInvalid sentiment received: STRONGLY_POS, defaulting to NEUTRAL\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  32%|\u2588\u2588\u2588\u258f      | 16/50 [02:13<03:42,  6.55s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nInvalid sentiment received: STRONGLY_N, defaulting to NEUTRAL\nRate limit hit - waiting 10s before retry (2/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  36%|\u2588\u2588\u2588\u258c      | 18/50 [02:30<03:44,  7.03s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\n\nRate limit hit - waiting 10s before retry (2/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  38%|\u2588\u2588\u2588\u258a      | 19/50 [02:48<05:11, 10.06s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nInvalid sentiment received: STRONGLY_N, defaulting to NEUTRAL\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  40%|\u2588\u2588\u2588\u2588      | 20/50 [02:54<04:33,  9.11s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 10s before retry (2/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  42%|\u2588\u2588\u2588\u2588\u258f     | 21/50 [03:11<05:29, 11.38s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  44%|\u2588\u2588\u2588\u2588\u258d     | 22/50 [03:17<04:34,  9.81s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Invalid sentiment received: STRONGLY_POS, defaulting to NEUTRAL\nRate limit hit - waiting 5s before retry (1/3)\nInvalid sentiment received: STRONGLY_POS, defaulting to NEUTRAL\nRate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  46%|\u2588\u2588\u2588\u2588\u258c     | 23/50 [03:24<03:58,  8.84s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)Rate limit hit - waiting 5s before retry (1/3)\n\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 10s before retry (2/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  48%|\u2588\u2588\u2588\u2588\u258a     | 24/50 [03:41<04:53, 11.28s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nInvalid sentiment received: STRONGLY_N, defaulting to NEUTRAL\nRate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  50%|\u2588\u2588\u2588\u2588\u2588     | 25/50 [03:47<04:06,  9.85s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nInvalid sentiment received: STRONGLY_POS, defaulting to NEUTRAL\nRate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 26/50 [03:54<03:33,  8.90s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)Rate limit hit - waiting 5s before retry (1/3)\n\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 10s before retry (2/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 27/50 [04:11<04:22, 11.40s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 28/50 [04:17<03:36,  9.82s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 29/50 [04:24<03:04,  8.76s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 10s before retry (2/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 30/50 [04:40<03:41, 11.06s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 10s before retry (2/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 31/50 [04:57<04:01, 12.72s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Invalid sentiment received: STRONGLY_POS, defaulting to NEUTRAL\nRate limit hit - waiting 5s before retry (1/3)Rate limit hit - waiting 5s before retry (1/3)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 32/50 [05:03<03:13, 10.75s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)Rate limit hit - waiting 5s before retry (1/3)\n\nRate limit hit - waiting 5s before retry (1/3)\nInvalid sentiment received: STRONGLY_POS, defaulting to NEUTRAL\nRate limit hit - waiting 10s before retry (2/3)Rate limit hit - waiting 10s before retry (2/3)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 33/50 [05:20<03:33, 12.58s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 34/50 [05:27<02:53, 10.85s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 35/50 [05:33<02:23,  9.54s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 36/50 [05:39<01:59,  8.53s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 37/50 [05:46<01:43,  7.93s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\n\nRate limit hit - waiting 10s before retry (2/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 38/50 [06:03<02:07, 10.61s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Invalid sentiment received: STRONGLY_N, defaulting to NEUTRAL\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 39/50 [06:04<01:25,  7.76s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Invalid sentiment received: STRONGLY_N, defaulting to NEUTRAL\nRate limit hit - waiting 5s before retry (1/3)Rate limit hit - waiting 5s before retry (1/3)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 40/50 [06:12<01:18,  7.83s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 41/50 [06:18<01:05,  7.33s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 42/50 [06:25<00:58,  7.26s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 43/50 [06:31<00:48,  6.88s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nInvalid sentiment received: STRONGLY_N, defaulting to NEUTRAL\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 44/50 [06:37<00:39,  6.63s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 46/50 [06:44<00:18,  4.70s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 10s before retry (2/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 47/50 [07:01<00:25,  8.35s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nRate limit hit - waiting 5s before retry (1/3)\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 48/50 [07:07<00:15,  7.93s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Invalid sentiment received: STRONGLY_N, defaulting to NEUTRAL\nRate limit hit - waiting 5s before retry (1/3)\nInvalid sentiment received: STRONGLY_POS, defaulting to NEUTRAL\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 49/50 [07:14<00:07,  7.43s/it]"}, {"name": "stdout", "output_type": "stream", "text": "Rate limit hit - waiting 5s before retry (1/3)\nInvalid sentiment received: STRONGLY_N, defaulting to NEUTRAL\n"}, {"name": "stderr", "output_type": "stream", "text": "Processing tweet batches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [07:20<00:00,  8.81s/it]\n"}, {"ename": "AttributeError", "evalue": "'DataFrame' object has no attribute 'with_column'", "output_type": "error", "traceback": ["\u001b[31m---------------------------------------------------------------------------\u001b[39m", "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)", "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m sentiments = process_tweets(tweets, batch_size=\u001b[32m4\u001b[39m, max_workers=\u001b[32m8\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Add sentiments to the DataFrame\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m sample_df = \u001b[43msample_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_column\u001b[49m(pl.Series(name=\u001b[33m'\u001b[39m\u001b[33msentiment\u001b[39m\u001b[33m'\u001b[39m, values=sentiments))\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[32m      9\u001b[39m sample_df.select([tweet_column, \u001b[33m'\u001b[39m\u001b[33msentiment\u001b[39m\u001b[33m'\u001b[39m]).head(\u001b[32m10\u001b[39m)\n", "\u001b[31mAttributeError\u001b[39m: 'DataFrame' object has no attribute 'with_column'"]}], "source": "# Process a subset of tweets\ntweets = sample_df[tweet_column].to_list()\nsentiments = process_tweets(tweets, batch_size=4, max_workers=8)\n\n# Add sentiments to the DataFrame\nsample_df = sample_df.with_columnss(pl.Series(name='sentiment', values=sentiments))\n\n# Display the results\nsample_df.select([tweet_column, 'sentiment']).head(10)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Save the labeled data\noutput_path = \"../data/labeled_stock_tweets.csv\"\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\nsample_df.write_csv(output_path)\nprint(f\"\\nSaved labeled data to {output_path}\")\n\n# Display some examples of each sentiment\nprint(\"\\nExamples for each sentiment:\")\nfor sentiment in ['STRONGLY_POSITIVE', 'POSITIVE', 'NEUTRAL', 'NEGATIVE', 'STRONGLY_NEGATIVE']:\n    examples = sample_df.filter(pl.col(\"sentiment\") == sentiment).sample(1, seed=42)\n    if examples.shape[0] > 0:\n        print(f\"\\n{sentiment}:\\n{examples[0, tweet_column]}\")"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.0"}}, "nbformat": 4, "nbformat_minor": 4}