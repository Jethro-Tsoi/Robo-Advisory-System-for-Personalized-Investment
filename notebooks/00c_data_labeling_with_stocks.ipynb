{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Tweet Sentiment Labeling with Gemini (Stock-Focused)\n",
    "\n",
    "This notebook handles the labeling of financial tweets with verified stock symbols using Google's Gemini API:\n",
    "1. Load the preprocessed CSV files with NER and stock symbol information\n",
    "2. Process tweets with verified stock symbols through Gemini\n",
    "3. Save labeled data\n",
    "\n",
    "Sentiment Labels:\n",
    "- STRONGLY_POSITIVE\n",
    "- POSITIVE\n",
    "- NEUTRAL\n",
    "- NEGATIVE\n",
    "- STRONGLY_NEGATIVE\n",
    "- NOT_RELATED\n",
    "- UNCERTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "from glob import glob\n",
    "import google.generativeai as genai\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Configure Gemini API\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data\n",
    "\n",
    "Load the data with NER results and verified stock symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the data with verified stock symbols\n",
    "df = pd.read_csv('../data/tweets_with_verified_stocks.csv')\n",
    "print(f\"Loaded {len(df)} tweets with verified stock symbols\")\n",
    "\n",
    "# Convert string representations of lists to actual lists\n",
    "def convert_str_to_list(str_list):\n",
    "    if pd.isna(str_list):\n",
    "        return None\n",
    "    try:\n",
    "        return ast.literal_eval(str_list)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply conversion to list columns\n",
    "list_columns = ['entity_types', 'entity_values', 'potential_symbols', 'verified_stock_symbols']\n",
    "for col in list_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(convert_str_to_list)\n",
    "\n",
    "# Display a sample of the data\n",
    "df[['cleaned_text', 'entity_types', 'verified_stock_symbols']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Sentiment Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def setup_prompt():\n",
    "    \"\"\"Configure the system prompt for Gemini with stock symbol context\"\"\"\n",
    "    return \"\"\"\n",
    "    You are a financial sentiment analyzer. Classify the given tweet's sentiment into one of these categories, specifically in the context of the mentioned stock symbol(s):\n",
    "\n",
    "    STRONGLY_POSITIVE - Very bullish, highly confident optimistic outlook for the stock(s)\n",
    "    POSITIVE - Generally optimistic, bullish view of the stock(s)\n",
    "    NEUTRAL - Factual, balanced, or no clear sentiment about the stock(s)\n",
    "    NEGATIVE - Generally pessimistic, bearish view of the stock(s)\n",
    "    STRONGLY_NEGATIVE - Very bearish, highly confident pessimistic outlook for the stock(s)\n",
    "    NOT_RELATED - Mentions the stock symbol but not related to its financial performance\n",
    "    UNCERTAIN - Ambiguous or unclear sentiment about the stock(s)\n",
    "\n",
    "    Examples:\n",
    "    \"Breaking: $AAPL doubles profit forecast!\" -> STRONGLY_POSITIVE\n",
    "    \"Expecting modest gains for $MSFT next quarter\" -> POSITIVE\n",
    "    \"$AMZN closed at 3500\" -> NEUTRAL\n",
    "    \"Concerned about $TSLA's rising costs\" -> NEGATIVE\n",
    "    \"$NFLX crash incoming, sell everything!\" -> STRONGLY_NEGATIVE\n",
    "    \"I'm watching $DIS movie on my new TV\" -> NOT_RELATED\n",
    "    \"Something might happen with $GME\" -> UNCERTAIN\n",
    "\n",
    "    Tweet to analyze: {text}\n",
    "    Stock symbols mentioned: {symbols}\n",
    "\n",
    "    Format: Return only one word from: STRONGLY_POSITIVE, POSITIVE, NEUTRAL, NEGATIVE, STRONGLY_NEGATIVE, NOT_RELATED, UNCERTAIN\n",
    "    \"\"\"\n",
    "\n",
    "def get_sentiment(text, symbols, retries=3):\n",
    "    \"\"\"Get sentiment from Gemini with retry logic, focusing on stock symbols\"\"\"\n",
    "    prompt = setup_prompt().format(text=text, symbols=symbols)\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            sentiment = response.text.strip().upper()\n",
    "            \n",
    "            # Validate the response\n",
    "            valid_labels = [\n",
    "                'STRONGLY_POSITIVE', 'POSITIVE', 'NEUTRAL', 'NEGATIVE',\n",
    "                'STRONGLY_NEGATIVE', 'NOT_RELATED', 'UNCERTAIN'\n",
    "            ]\n",
    "            \n",
    "            if sentiment in valid_labels:\n",
    "                return sentiment\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid sentiment: {sentiment}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\"Error processing text: {text}\\nError: {str(e)}\")\n",
    "                return 'UNCERTAIN'\n",
    "            time.sleep(1)  # Wait before retry\n",
    "    \n",
    "    return 'UNCERTAIN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Sentiment Labeling on a Small Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test on a small sample\n",
    "sample_df = df.head(5).copy()\n",
    "sample_sentiments = []\n",
    "\n",
    "for _, row in sample_df.iterrows():\n",
    "    text = row['cleaned_text']\n",
    "    symbols = row['verified_stock_symbols']\n",
    "    sentiment = get_sentiment(text, symbols)\n",
    "    sample_sentiments.append(sentiment)\n",
    "    print(f\"Text: {text[:100]}...\\nSymbols: {symbols}\\nSentiment: {sentiment}\\n---\")\n",
    "    time.sleep(0.5)  # Rate limiting\n",
    "\n",
    "sample_df['sentiment'] = sample_sentiments\n",
    "sample_df[['cleaned_text', 'verified_stock_symbols', 'sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process All Tweets with Verified Stock Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_dataframe(input_df, batch_size=50):\n",
    "    \"\"\"Process the dataframe in batches to avoid rate limiting\"\"\"\n",
    "    result_df = input_df.copy()\n",
    "    sentiments = []\n",
    "    \n",
    "    # Skip if already processed\n",
    "    if 'sentiment' in result_df.columns and not result_df['sentiment'].isnull().all():\n",
    "        print(\"Data already processed\")\n",
    "        return result_df\n",
    "    \n",
    "    total_rows = len(result_df)\n",
    "    \n",
    "    for i in tqdm(range(0, total_rows, batch_size), desc=\"Processing batches\"):\n",
    "        end_idx = min(i + batch_size, total_rows)\n",
    "        batch = result_df.iloc[i:end_idx]\n",
    "        \n",
    "        batch_sentiments = []\n",
    "        for _, row in batch.iterrows():\n",
    "            text = row['cleaned_text']\n",
    "            symbols = row['verified_stock_symbols']\n",
    "            sentiment = get_sentiment(text, symbols)\n",
    "            batch_sentiments.append(sentiment)\n",
    "            time.sleep(0.2)  # Rate limiting\n",
    "        \n",
    "        sentiments.extend(batch_sentiments)\n",
    "        # Save intermediate results after each batch\n",
    "        temp_df = result_df.copy()\n",
    "        temp_df.loc[:end_idx-1, 'sentiment'] = sentiments\n",
    "        temp_df.to_csv('../data/stock_tweets_labeled_in_progress.csv', index=False)\n",
    "    \n",
    "    result_df['sentiment'] = sentiments\n",
    "    return result_df\n",
    "\n",
    "# Process all data\n",
    "labeled_df = process_dataframe(df)\n",
    "\n",
    "# Save final results\n",
    "labeled_df.to_csv('../data/stock_tweets_labeled.csv', index=False)\n",
    "print(f\"Saved labeled data to '../data/stock_tweets_labeled.csv'\")\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "print(labeled_df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Filter for Training Dataset\n",
    "\n",
    "Create a final dataset that excludes NOT_RELATED tweets for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a filtered dataset excluding NOT_RELATED tweets\n",
    "filtered_df = labeled_df[labeled_df['sentiment'] != 'NOT_RELATED'].copy()\n",
    "filtered_df.to_csv('../data/stock_tweets_for_training.csv', index=False)\n",
    "print(f\"Saved {len(filtered_df)} tweets for training to '../data/stock_tweets_for_training.csv'\")\n",
    "\n",
    "# Print statistics for the filtered dataset\n",
    "print(\"\\nSentiment Distribution (Training Dataset):\")\n",
    "print(filtered_df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Results by Stock Symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Explode the dataframe to analyze by individual stock symbol\n",
    "exploded_df = labeled_df.explode('verified_stock_symbols').dropna(subset=['verified_stock_symbols'])\n",
    "exploded_df = exploded_df.rename(columns={'verified_stock_symbols': 'stock_symbol'})\n",
    "\n",
    "# Count tweets by stock symbol and sentiment\n",
    "symbol_sentiment_counts = exploded_df.groupby(['stock_symbol', 'sentiment']).size().unstack(fill_value=0)\n",
    "\n",
    "# Show top stocks by tweet count\n",
    "top_stocks = exploded_df['stock_symbol'].value_counts().head(20)\n",
    "print(\"Top stocks by tweet count:\")\n",
    "print(top_stocks)\n",
    "\n",
    "# Sentiment distribution for top 5 stocks\n",
    "top_5_stocks = top_stocks.index[:5]\n",
    "print(\"\\nSentiment distribution for top 5 stocks:\")\n",
    "for stock in top_5_stocks:\n",
    "    stock_data = exploded_df[exploded_df['stock_symbol'] == stock]\n",
    "    print(f\"\\n{stock} sentiment distribution:\")\n",
    "    print(stock_data['sentiment'].value_counts())\n",
    "\n",
    "# Save the exploded dataframe for further analysis\n",
    "exploded_df.to_csv('../data/stock_tweets_by_symbol.csv', index=False)\n",
    "print(f\"\\nSaved expanded data by stock symbol to '../data/stock_tweets_by_symbol.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "This notebook has processed financial tweets with verified stock symbols, labeled their sentiment using Gemini, and prepared datasets for further analysis and model training.\n",
    "\n",
    "Files created:\n",
    "1. `stock_tweets_labeled.csv` - All tweets with verified stock symbols and their sentiment\n",
    "2. `stock_tweets_for_training.csv` - Filtered dataset excluding NOT_RELATED tweets, ready for model training\n",
    "3. `stock_tweets_by_symbol.csv` - Expanded dataset for analysis by individual stock symbol\n",
    "\n",
    "Next steps:\n",
    "1. Use `stock_tweets_for_training.csv` for model training (Gamma 3, Gemma 3, or FinBERT)\n",
    "2. Analyze sentiment by stock symbol for insights\n",
    "3. Develop predictive models based on stock-specific sentiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}