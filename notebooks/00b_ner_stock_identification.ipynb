{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Tweet NER and Stock Symbol Identification\n",
    "\n",
    "This notebook processes the raw tweet CSV files to:\n",
    "1. Apply BERT-based NER to identify entity types in tweets\n",
    "2. Extract potential stock symbols and match them with actual stock symbols\n",
    "3. Prepare the data for sentiment labeling\n",
    "\n",
    "These preprocessing steps will help identify tweets that are discussing specific stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n",
    "\n",
    "First, we need to install the necessary packages for NER and stock symbol lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting yfinance\n",
      "  Downloading yfinance-0.2.55-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: filelock in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from yfinance) (2.2.3)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Using cached multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from yfinance) (4.3.7)\n",
      "Requirement already satisfied: pytz>=2022.5 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from yfinance) (2025.1)\n",
      "Collecting frozendict>=2.3.4 (from yfinance)\n",
      "  Downloading frozendict-2.4.6-py311-none-any.whl.metadata (23 kB)\n",
      "Collecting peewee>=3.16.2 (from yfinance)\n",
      "  Using cached peewee-3.17.9.tar.gz (3.0 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.11.1 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from yfinance) (4.13.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "Using cached transformers-4.50.2-py3-none-any.whl (10.2 MB)\n",
      "Downloading yfinance-0.2.55-py2.py3-none-any.whl (109 kB)\n",
      "Downloading frozendict-2.4.6-py311-none-any.whl (16 kB)\n",
      "Using cached multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Building wheels for collected packages: peewee\n",
      "  Building wheel for peewee (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peewee: filename=peewee-3.17.9-cp311-cp311-macosx_11_0_arm64.whl size=264594 sha256=31bf562b81d4ca1a53fb13f326be0da982bc1dbf9dca111a0d17b72db9e4f0d9\n",
      "  Stored in directory: /Users/jethrotsoi/Library/Caches/pip/wheels/f4/14/e4/50c88c865833085aeb91e2bd40e3a683ff434806386b8ee7bc\n",
      "Successfully built peewee\n",
      "Installing collected packages: peewee, multitasking, safetensors, frozendict, yfinance, transformers\n",
      "Successfully installed frozendict-2.4.6 multitasking-0.0.11 peewee-3.17.9 safetensors-0.5.3 transformers-4.50.2 yfinance-0.2.55\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers yfinance requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# Import the newly installed packages\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import yfinance as yf",
    "\n# Create output directory if it doesn't exist\nos.makedirs('../data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Raw Tweet Files\n",
    "\n",
    "Load all the raw CSV files from the data/tweets directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61/61 [00:00<00:00, 135.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets loaded: 46656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>media</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>profile_image_url</th>\n",
       "      <th>in_reply_to</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>quoted_status</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>bookmark_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>views_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>bookmarked</th>\n",
       "      <th>url</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1476040410539560960</td>\n",
       "      <td>2021-12-29 12:00:32 +08:00</td>\n",
       "      <td>Savings is overated\\nIncreasing income is unde...</td>\n",
       "      <td>[]</td>\n",
       "      <td>MashraniVivek</td>\n",
       "      <td>Vivek Mashrani, CFA</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/157292460...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1747</td>\n",
       "      <td>145</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://twitter.com/MashraniVivek/status/14760...</td>\n",
       "      <td>row_28_mashranivivek.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1756238277839884344</td>\n",
       "      <td>2024-02-10 16:46:49 +08:00</td>\n",
       "      <td>Ever felt like you're riding a rollercoaster w...</td>\n",
       "      <td>[{\"type\":\"photo\",\"url\":\"https://t.co/VRqP4DxfZ...</td>\n",
       "      <td>MashraniVivek</td>\n",
       "      <td>Vivek Mashrani, CFA</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/157292460...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6404.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://twitter.com/MashraniVivek/status/17562...</td>\n",
       "      <td>row_28_mashranivivek.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1756238318755365117</td>\n",
       "      <td>2024-02-10 16:46:59 +08:00</td>\n",
       "      <td>8. Strategic Allocation Across Sectors:\\n\\nAll...</td>\n",
       "      <td>[]</td>\n",
       "      <td>MashraniVivek</td>\n",
       "      <td>Vivek Mashrani, CFA</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/157292460...</td>\n",
       "      <td>1.756238e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>506.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://twitter.com/MashraniVivek/status/17562...</td>\n",
       "      <td>row_28_mashranivivek.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1756238321980768720</td>\n",
       "      <td>2024-02-10 16:46:59 +08:00</td>\n",
       "      <td>9. Consider Individual Risk Tolerance:\\n\\nTail...</td>\n",
       "      <td>[]</td>\n",
       "      <td>MashraniVivek</td>\n",
       "      <td>Vivek Mashrani, CFA</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/157292460...</td>\n",
       "      <td>1.756238e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2685.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://twitter.com/MashraniVivek/status/17562...</td>\n",
       "      <td>row_28_mashranivivek.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1756238325155803182</td>\n",
       "      <td>2024-02-10 16:47:00 +08:00</td>\n",
       "      <td>10. Implement a Systematic Approach to Manage ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>MashraniVivek</td>\n",
       "      <td>Vivek Mashrani, CFA</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/157292460...</td>\n",
       "      <td>1.756238e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2655.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://twitter.com/MashraniVivek/status/17562...</td>\n",
       "      <td>row_28_mashranivivek.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                  created_at                                          full_text                                              media    screen_name                 name                                  profile_image_url   in_reply_to  retweeted_status  quoted_status  favorite_count  retweet_count  bookmark_count  quote_count  reply_count  views_count  favorited  retweeted  bookmarked                                                url               source_file\n",
       "0  1476040410539560960  2021-12-29 12:00:32 +08:00  Savings is overated\\nIncreasing income is unde...                                                 []  MashraniVivek  Vivek Mashrani, CFA  https://pbs.twimg.com/profile_images/157292460...           NaN               NaN            NaN            1747            145              39           14           40          NaN      False      False       False  https://twitter.com/MashraniVivek/status/14760...  row_28_mashranivivek.csv\n",
       "1  1756238277839884344  2024-02-10 16:46:49 +08:00  Ever felt like you're riding a rollercoaster w...  [{\"type\":\"photo\",\"url\":\"https://t.co/VRqP4DxfZ...  MashraniVivek  Vivek Mashrani, CFA  https://pbs.twimg.com/profile_images/157292460...           NaN               NaN            NaN              31              5              14            0            5       6404.0      False      False       False  https://twitter.com/MashraniVivek/status/17562...  row_28_mashranivivek.csv\n",
       "2  1756238318755365117  2024-02-10 16:46:59 +08:00  8. Strategic Allocation Across Sectors:\\n\\nAll...                                                 []  MashraniVivek  Vivek Mashrani, CFA  https://pbs.twimg.com/profile_images/157292460...  1.756238e+18               NaN            NaN               3              1               0            0            1        506.0      False      False       False  https://twitter.com/MashraniVivek/status/17562...  row_28_mashranivivek.csv\n",
       "3  1756238321980768720  2024-02-10 16:46:59 +08:00  9. Consider Individual Risk Tolerance:\\n\\nTail...                                                 []  MashraniVivek  Vivek Mashrani, CFA  https://pbs.twimg.com/profile_images/157292460...  1.756238e+18               NaN            NaN               2              1               0            0            1       2685.0      False      False       False  https://twitter.com/MashraniVivek/status/17562...  row_28_mashranivivek.csv\n",
       "4  1756238325155803182  2024-02-10 16:47:00 +08:00  10. Implement a Systematic Approach to Manage ...                                                 []  MashraniVivek  Vivek Mashrani, CFA  https://pbs.twimg.com/profile_images/157292460...  1.756238e+18               NaN            NaN               4              2               0            0            0       2655.0      False      False       False  https://twitter.com/MashraniVivek/status/17562...  row_28_mashranivivek.csv"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_tweet_files(data_dir='../data/tweets/'):\n    \"\"\"Load all row_*.csv files from the specified directory\"\"\"\n    # Check if directory exists\n    if not os.path.exists(data_dir):\n        print(f\"Warning: Directory {data_dir} does not exist. Creating it.\")\n        os.makedirs(data_dir, exist_ok=True)\n        return pd.DataFrame()\n        \n    all_files = glob(os.path.join(data_dir, 'row_*.csv'))\n    \n    if not all_files:\n        print(f\"Warning: No row_*.csv files found in {data_dir}\")\n        return pd.DataFrame()\n    \n    dataframes = []\n    for file in tqdm(all_files, desc='Loading files'):\n        try:\n            df = pd.read_csv(file)\n            # Add source file name as a column\n            df['source_file'] = os.path.basename(file)\n            dataframes.append(df)\n        except Exception as e:\n            print(f\"Error loading {file}: {str(e)}\")\n    \n    if dataframes:\n        return pd.concat(dataframes, ignore_index=True)\n    else:\n        return pd.DataFrame()\n\n# Load all tweet data\ndf = load_tweet_files()\nprint(f\"Total tweets loaded: {len(df)}\")\ndf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Cleaning for NER\n",
    "\n",
    "Clean the tweet text before applying NER, but preserve mentions of potential stock symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets after cleaning: 45875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Savings is overated\\nIncreasing income is unde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ever felt like you're riding a rollercoaster w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8. Strategic Allocation Across Sectors:\\n\\nAll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9. Consider Individual Risk Tolerance:\\n\\nTail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10. Implement a Systematic Approach to Manage ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text\n",
       "0  Savings is overated\\nIncreasing income is unde...\n",
       "1  Ever felt like you're riding a rollercoaster w...\n",
       "2  8. Strategic Allocation Across Sectors:\\n\\nAll...\n",
       "3  9. Consider Individual Risk Tolerance:\\n\\nTail...\n",
       "4  10. Implement a Systematic Approach to Manage ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_tweet_for_ner(text):\n",
    "    \"\"\"Clean tweet text for NER while preserving potential stock symbols\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Replace extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Clean the tweets\n",
    "df['cleaned_text'] = df['full_text'].apply(clean_tweet_for_ner)\n",
    "\n",
    "# Remove empty tweets\n",
    "df = df[df['cleaned_text'].str.len() > 0].reset_index(drop=True)\n",
    "print(f\"Tweets after cleaning: {len(df)}\")\n",
    "df[['cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply BERT-based NER\n",
    "\n",
    "Use BERT-based Named Entity Recognition to identify entities in the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load NER pipeline\u001b[39;00m\n\u001b[32m      2\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mdslim/bert-base-NER\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mAutoModelForTokenClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mdslim/bert-base-NER\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m ner_pipeline = pipeline(\u001b[33m\"\u001b[39m\u001b[33mner\u001b[39m\u001b[33m\"\u001b[39m, model=model, tokenizer=tokenizer, aggregation_strategy=\u001b[33m\"\u001b[39m\u001b[33msimple\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PIRAS/lib/python3.11/site-packages/transformers/utils/import_utils.py:1849\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   1847\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1848\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m1849\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PIRAS/lib/python3.11/site-packages/transformers/utils/import_utils.py:1837\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   1835\u001b[39m failed = [msg.format(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[32m   1836\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m1837\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
      "\u001b[31mImportError\u001b[39m: \nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "# Load NER pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply NER to all tweets (this may take some time)\nprint(\"Applying NER to all tweets. This may take a while...\")\n\n# Check for checkpoint file\ncheckpoint_file = '../data/ner_checkpoint.csv'\nif os.path.exists(checkpoint_file):\n    try:\n        checkpoint_df = pd.read_csv(checkpoint_file)\n        if 'entity_types' in checkpoint_df.columns and 'entity_values' in checkpoint_df.columns:\n            print(f\"Loading NER results from checkpoint file ({len(checkpoint_df)} rows)\")\n            df = checkpoint_df\n            ner_complete = True\n        else:\n            ner_complete = False\n    except Exception as e:\n        print(f\"Error loading checkpoint: {str(e)}\")\n        ner_complete = False\nelse:\n    ner_complete = False\n\nif not ner_complete:\n    # We'll process in smaller batches to prevent memory issues\n    batch_size = 100\n    num_batches = (len(df) + batch_size - 1) // batch_size\n    \n    all_entity_types = []\n    all_entity_values = []\n    \n    for i in tqdm(range(num_batches), desc=\"Processing batches\"):\n        start_idx = i * batch_size\n        end_idx = min((i + 1) * batch_size, len(df))\n        \n        batch_texts = df['cleaned_text'].iloc[start_idx:end_idx].tolist()\n        batch_results = []\n        \n        for text in batch_texts:\n            batch_results.append(apply_ner(text, ner_pipeline))\n        \n        all_entity_types.extend([r[0] for r in batch_results])\n        all_entity_values.extend([r[1] for r in batch_results])\n        \n        # Save checkpoint every 5 batches\n        if (i+1) % 5 == 0 or i == num_batches - 1:\n            temp_df = df.copy()\n            temp_types = all_entity_types + [[] for _ in range(len(df) - len(all_entity_types))]\n            temp_values = all_entity_values + [[] for _ in range(len(df) - len(all_entity_values))]\n            temp_df['entity_types'] = temp_types\n            temp_df['entity_values'] = temp_values\n            temp_df.to_csv(checkpoint_file, index=False)\n            print(f\"Saved checkpoint after batch {i+1}/{num_batches}\")\n    \n    # Add results to dataframe\n    df['entity_types'] = all_entity_types\n    df['entity_values'] = all_entity_values\n\nprint(\"NER processing complete!\")\ndf[['cleaned_text', 'entity_types', 'entity_values']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply NER to all tweets (this may take some time)\n",
    "print(\"Applying NER to all tweets. This may take a while...\")\n",
    "\n",
    "# We'll process in smaller batches to prevent memory issues\n",
    "batch_size = 100\n",
    "num_batches = (len(df) + batch_size - 1) // batch_size\n",
    "\n",
    "all_entity_types = []\n",
    "all_entity_values = []\n",
    "\n",
    "for i in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(df))\n",
    "    \n",
    "    batch_texts = df['cleaned_text'].iloc[start_idx:end_idx].tolist()\n",
    "    batch_results = []\n",
    "    \n",
    "    for text in batch_texts:\n",
    "        batch_results.append(apply_ner(text, ner_pipeline))\n",
    "    \n",
    "    all_entity_types.extend([r[0] for r in batch_results])\n",
    "    all_entity_values.extend([r[1] for r in batch_results])\n",
    "\n",
    "# Add results to dataframe\n",
    "df['entity_types'] = all_entity_types\n",
    "df['entity_values'] = all_entity_values\n",
    "\n",
    "print(\"NER processing complete!\")\n",
    "df[['cleaned_text', 'entity_types', 'entity_values']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract and Verify Stock Symbols\n",
    "\n",
    "Extract potential stock symbols from tweets and verify them against actual stock symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_stock_symbol(symbol, max_retries=3):\n    \"\"\"Verify if a given string is a valid stock symbol using yfinance with retry logic\"\"\"\n    for attempt in range(max_retries):\n        try:\n            ticker = yf.Ticker(symbol)\n            # Try to get some basic info to verify it's valid\n            info = ticker.info\n            \n            # Check if we got valid info (if the ticker doesn't exist, this will be empty or have minimal data)\n            if 'symbol' in info and info['symbol'] == symbol:\n                return True\n            return False\n        except Exception as e:\n            if attempt < max_retries - 1:\n                print(f\"Retry {attempt+1} for {symbol}: {str(e)}\")\n                time.sleep(1)  # Wait before retrying\n            else:\n                print(f\"Failed to verify {symbol} after {max_retries} attempts: {str(e)}\")\n                return False\n\ndef get_verified_symbols(symbols_list):\n    \"\"\"From a list of potential symbols, return the valid ones\"\"\"\n    if not symbols_list:\n        return None\n    \n    verified = []\n    for symbol in symbols_list:\n        if verify_stock_symbol(symbol):\n            verified.append(symbol)\n    \n    return verified if verified else None\n\n# Test on a small sample first\nsample_symbols = df[df['potential_symbols'].apply(lambda x: len(x) > 0)]['potential_symbols'].head(5)\nfor symbols in sample_symbols:\n    verified = get_verified_symbols(symbols)\n    print(f\"Potential: {symbols}, Verified: {verified}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply verification to all potential symbols\nprint(\"Verifying stock symbols. This may take a while...\")\n\n# Check for checkpoint file\nsymbols_checkpoint_file = '../data/symbols_checkpoint.csv'\nif os.path.exists(symbols_checkpoint_file):\n    try:\n        symbols_df = pd.read_csv(symbols_checkpoint_file)\n        if 'verified_stock_symbols' in symbols_df.columns:\n            print(f\"Loading verified symbols from checkpoint file ({len(symbols_df)} rows)\")\n            df = symbols_df\n            symbols_complete = True\n        else:\n            symbols_complete = False\n    except Exception as e:\n        print(f\"Error loading symbols checkpoint: {str(e)}\")\n        symbols_complete = False\nelse:\n    symbols_complete = False\n\nif not symbols_complete:\n    # Create a cache to avoid repeated API calls for the same symbol\n    symbol_cache = {}\n    \n    def get_verified_symbols_with_cache(symbols_list):\n        \"\"\"Use cache to speed up symbol verification\"\"\"\n        if not symbols_list:\n            return None\n        \n        verified = []\n        for symbol in symbols_list:\n            if symbol in symbol_cache:\n                if symbol_cache[symbol]:\n                    verified.append(symbol)\n            else:\n                is_valid = verify_stock_symbol(symbol)\n                symbol_cache[symbol] = is_valid\n                if is_valid:\n                    verified.append(symbol)\n        \n        return verified if verified else None\n    \n    # Process in batches and save checkpoints\n    batch_size = 100\n    rows_with_symbols = df[df['potential_symbols'].apply(lambda x: len(x) > 0)]\n    \n    if len(rows_with_symbols) > 0:\n        print(f\"Processing {len(rows_with_symbols)} tweets with potential symbols\")\n        \n        for i, idx in enumerate(tqdm(rows_with_symbols.index, desc=\"Verifying symbols\")):\n            symbols = df.loc[idx, 'potential_symbols']\n            verified = get_verified_symbols_with_cache(symbols)\n            df.at[idx, 'verified_stock_symbols'] = verified\n            \n            # Save checkpoint periodically\n            if (i+1) % batch_size == 0 or i == len(rows_with_symbols) - 1:\n                df.to_csv(symbols_checkpoint_file, index=False)\n                print(f\"Saved checkpoint after processing {i+1}/{len(rows_with_symbols)} rows\")\n    else:\n        print(\"No tweets with potential stock symbols found\")\n\ndf[['cleaned_text', 'potential_symbols', 'verified_stock_symbols']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply verification to all potential symbols\n",
    "print(\"Verifying stock symbols. This may take a while...\")\n",
    "\n",
    "# Create a cache to avoid repeated API calls for the same symbol\n",
    "symbol_cache = {}\n",
    "\n",
    "def get_verified_symbols_with_cache(symbols_list):\n",
    "    \"\"\"Use cache to speed up symbol verification\"\"\"\n",
    "    if not symbols_list:\n",
    "        return None\n",
    "    \n",
    "    verified = []\n",
    "    for symbol in symbols_list:\n",
    "        if symbol in symbol_cache:\n",
    "            if symbol_cache[symbol]:\n",
    "                verified.append(symbol)\n",
    "        else:\n",
    "            is_valid = verify_stock_symbol(symbol)\n",
    "            symbol_cache[symbol] = is_valid\n",
    "            if is_valid:\n",
    "                verified.append(symbol)\n",
    "    \n",
    "    return verified if verified else None\n",
    "\n",
    "# Apply to all rows with potential symbols\n",
    "tqdm.pandas(desc=\"Verifying symbols\")\n",
    "df['verified_stock_symbols'] = df['potential_symbols'].progress_apply(get_verified_symbols_with_cache)\n",
    "df[['cleaned_text', 'potential_symbols', 'verified_stock_symbols']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all processed tweets\nos.makedirs('../data', exist_ok=True)  # Ensure directory exists\ndf.to_csv('../data/tweets_with_ner_and_stocks.csv', index=False)\nprint(f\"Saved all processed tweets to '../data/tweets_with_ner_and_stocks.csv'\")\n\n# Create a filtered dataset with only tweets that mention verified stock symbols\nif 'verified_stock_symbols' in df.columns:\n    stock_tweets_df = df[df['verified_stock_symbols'].notnull()].copy()\n    stock_tweets_df.to_csv('../data/tweets_with_verified_stocks.csv', index=False)\n    print(f\"Saved {len(stock_tweets_df)} tweets with verified stock symbols to '../data/tweets_with_verified_stocks.csv'\")\nelse:\n    print(\"Warning: No 'verified_stock_symbols' column found in the dataframe. Check the previous steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data for Labeling\n",
    "\n",
    "Save the processed data with NER results and verified stock symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all processed tweets\n",
    "df.to_csv('../data/tweets_with_ner_and_stocks.csv', index=False)\n",
    "print(f\"Saved all processed tweets to '../data/tweets_with_ner_and_stocks.csv'\")\n",
    "\n",
    "# Create a filtered dataset with only tweets that mention verified stock symbols\n",
    "stock_tweets_df = df[df['verified_stock_symbols'].notnull()].copy()\n",
    "stock_tweets_df.to_csv('../data/tweets_with_verified_stocks.csv', index=False)\n",
    "print(f\"Saved {len(stock_tweets_df)} tweets with verified stock symbols to '../data/tweets_with_verified_stocks.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "After this notebook, you can proceed to the sentiment labeling process using:\n",
    "1. The `tweets_with_verified_stocks.csv` file for tweets that mention specific stocks\n",
    "2. The `tweets_with_ner_and_stocks.csv` file for all tweets with NER information\n",
    "\n",
    "The sentiment labeling should now focus on the tweets with verified stock symbols."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}