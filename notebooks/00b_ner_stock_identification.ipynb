{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Tweet NER and Stock Symbol Identification\n",
    "\n",
    "This notebook processes the raw tweet CSV files to:\n",
    "1. Apply BERT-based NER to identify entity types in tweets\n",
    "2. Extract potential stock symbols and match them with actual stock symbols\n",
    "3. Prepare the data for sentiment labeling\n",
    "\n",
    "These preprocessing steps will help identify tweets that are discussing specific stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n",
    "\n",
    "First, we need to install the necessary packages for NER and stock symbol lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (4.50.2)\n",
      "Requirement already satisfied: yfinance in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (0.2.55)\n",
      "Requirement already satisfied: requests in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: filelock in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from yfinance) (4.3.7)\n",
      "Requirement already satisfied: pytz>=2022.5 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from yfinance) (2025.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from yfinance) (3.17.9)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from yfinance) (4.13.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from torchvision) (2.2.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers yfinance requests\n",
    "\n",
    "# Install PyTorch - required for the NER models\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER pipeline loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load NER pipeline\ntry:\n    # Try importing required PyTorch packages first to give better error messages\n    import torch\n    from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n    import yfinance as yf\n    \n    tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n    model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n    ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n    print(\"NER pipeline loaded successfully\")\n    skip_ner = False\nexcept ImportError as e:\n    print(f\"Error loading PyTorch dependencies: {str(e)}\")\n    print(\"Please install PyTorch with: pip install torch torchvision torchaudio\")\n    print(\"NER functionality will be skipped, but the notebook will continue with stock symbol extraction\")\n    # Set a flag to skip NER-related code\n    skip_ner = True\nexcept Exception as e:\n    print(f\"Error loading NER pipeline: {str(e)}\")\n    print(\"NER functionality will be skipped, but the notebook will continue with stock symbol extraction\")\n    # Set a flag to skip NER-related code\n    skip_ner = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Raw Tweet Files\n",
    "\n",
    "Load all the raw CSV files from the data/tweets directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61/61 [00:00<00:00, 122.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets loaded: 46656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>media</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>profile_image_url</th>\n",
       "      <th>in_reply_to</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>quoted_status</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>bookmark_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>views_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>bookmarked</th>\n",
       "      <th>url</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1476040410539560960</td>\n",
       "      <td>2021-12-29 12:00:32 +08:00</td>\n",
       "      <td>Savings is overated\\nIncreasing income is unde...</td>\n",
       "      <td>[]</td>\n",
       "      <td>MashraniVivek</td>\n",
       "      <td>Vivek Mashrani, CFA</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/157292460...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1747</td>\n",
       "      <td>145</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://twitter.com/MashraniVivek/status/14760...</td>\n",
       "      <td>row_28_mashranivivek.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1756238277839884344</td>\n",
       "      <td>2024-02-10 16:46:49 +08:00</td>\n",
       "      <td>Ever felt like you're riding a rollercoaster w...</td>\n",
       "      <td>[{\"type\":\"photo\",\"url\":\"https://t.co/VRqP4DxfZ...</td>\n",
       "      <td>MashraniVivek</td>\n",
       "      <td>Vivek Mashrani, CFA</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/157292460...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6404.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://twitter.com/MashraniVivek/status/17562...</td>\n",
       "      <td>row_28_mashranivivek.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1756238318755365117</td>\n",
       "      <td>2024-02-10 16:46:59 +08:00</td>\n",
       "      <td>8. Strategic Allocation Across Sectors:\\n\\nAll...</td>\n",
       "      <td>[]</td>\n",
       "      <td>MashraniVivek</td>\n",
       "      <td>Vivek Mashrani, CFA</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/157292460...</td>\n",
       "      <td>1.756238e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>506.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://twitter.com/MashraniVivek/status/17562...</td>\n",
       "      <td>row_28_mashranivivek.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1756238321980768720</td>\n",
       "      <td>2024-02-10 16:46:59 +08:00</td>\n",
       "      <td>9. Consider Individual Risk Tolerance:\\n\\nTail...</td>\n",
       "      <td>[]</td>\n",
       "      <td>MashraniVivek</td>\n",
       "      <td>Vivek Mashrani, CFA</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/157292460...</td>\n",
       "      <td>1.756238e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2685.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://twitter.com/MashraniVivek/status/17562...</td>\n",
       "      <td>row_28_mashranivivek.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1756238325155803182</td>\n",
       "      <td>2024-02-10 16:47:00 +08:00</td>\n",
       "      <td>10. Implement a Systematic Approach to Manage ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>MashraniVivek</td>\n",
       "      <td>Vivek Mashrani, CFA</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/157292460...</td>\n",
       "      <td>1.756238e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2655.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://twitter.com/MashraniVivek/status/17562...</td>\n",
       "      <td>row_28_mashranivivek.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                  created_at                                          full_text                                              media    screen_name                 name                                  profile_image_url   in_reply_to  retweeted_status  quoted_status  favorite_count  retweet_count  bookmark_count  quote_count  reply_count  views_count  favorited  retweeted  bookmarked                                                url               source_file\n",
       "0  1476040410539560960  2021-12-29 12:00:32 +08:00  Savings is overated\\nIncreasing income is unde...                                                 []  MashraniVivek  Vivek Mashrani, CFA  https://pbs.twimg.com/profile_images/157292460...           NaN               NaN            NaN            1747            145              39           14           40          NaN      False      False       False  https://twitter.com/MashraniVivek/status/14760...  row_28_mashranivivek.csv\n",
       "1  1756238277839884344  2024-02-10 16:46:49 +08:00  Ever felt like you're riding a rollercoaster w...  [{\"type\":\"photo\",\"url\":\"https://t.co/VRqP4DxfZ...  MashraniVivek  Vivek Mashrani, CFA  https://pbs.twimg.com/profile_images/157292460...           NaN               NaN            NaN              31              5              14            0            5       6404.0      False      False       False  https://twitter.com/MashraniVivek/status/17562...  row_28_mashranivivek.csv\n",
       "2  1756238318755365117  2024-02-10 16:46:59 +08:00  8. Strategic Allocation Across Sectors:\\n\\nAll...                                                 []  MashraniVivek  Vivek Mashrani, CFA  https://pbs.twimg.com/profile_images/157292460...  1.756238e+18               NaN            NaN               3              1               0            0            1        506.0      False      False       False  https://twitter.com/MashraniVivek/status/17562...  row_28_mashranivivek.csv\n",
       "3  1756238321980768720  2024-02-10 16:46:59 +08:00  9. Consider Individual Risk Tolerance:\\n\\nTail...                                                 []  MashraniVivek  Vivek Mashrani, CFA  https://pbs.twimg.com/profile_images/157292460...  1.756238e+18               NaN            NaN               2              1               0            0            1       2685.0      False      False       False  https://twitter.com/MashraniVivek/status/17562...  row_28_mashranivivek.csv\n",
       "4  1756238325155803182  2024-02-10 16:47:00 +08:00  10. Implement a Systematic Approach to Manage ...                                                 []  MashraniVivek  Vivek Mashrani, CFA  https://pbs.twimg.com/profile_images/157292460...  1.756238e+18               NaN            NaN               4              2               0            0            0       2655.0      False      False       False  https://twitter.com/MashraniVivek/status/17562...  row_28_mashranivivek.csv"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_tweet_files(data_dir='../data/tweets/'):\n",
    "    \"\"\"Load all row_*.csv files from the specified directory\"\"\"\n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Warning: Directory {data_dir} does not exist. Creating it.\")\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    all_files = glob(os.path.join(data_dir, 'row_*.csv'))\n",
    "    \n",
    "    if not all_files:\n",
    "        print(f\"Warning: No row_*.csv files found in {data_dir}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    dataframes = []\n",
    "    for file in tqdm(all_files, desc='Loading files'):\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            # Add source file name as a column\n",
    "            df['source_file'] = os.path.basename(file)\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {str(e)}\")\n",
    "    \n",
    "    if dataframes:\n",
    "        return pd.concat(dataframes, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load all tweet data\n",
    "df = load_tweet_files()\n",
    "print(f\"Total tweets loaded: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Cleaning for NER\n",
    "\n",
    "Clean the tweet text before applying NER, but preserve mentions of potential stock symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets after cleaning: 45875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Savings is overated\\nIncreasing income is unde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ever felt like you're riding a rollercoaster w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8. Strategic Allocation Across Sectors:\\n\\nAll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9. Consider Individual Risk Tolerance:\\n\\nTail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10. Implement a Systematic Approach to Manage ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text\n",
       "0  Savings is overated\\nIncreasing income is unde...\n",
       "1  Ever felt like you're riding a rollercoaster w...\n",
       "2  8. Strategic Allocation Across Sectors:\\n\\nAll...\n",
       "3  9. Consider Individual Risk Tolerance:\\n\\nTail...\n",
       "4  10. Implement a Systematic Approach to Manage ..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_tweet_for_ner(text):\n",
    "    \"\"\"Clean tweet text for NER while preserving potential stock symbols\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Replace extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Clean the tweets\n",
    "df['cleaned_text'] = df['full_text'].apply(clean_tweet_for_ner)\n",
    "\n",
    "# Remove empty tweets\n",
    "df = df[df['cleaned_text'].str.len() > 0].reset_index(drop=True)\n",
    "print(f\"Tweets after cleaning: {len(df)}\")\n",
    "df[['cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply BERT-based NER\n",
    "\n",
    "Use BERT-based Named Entity Recognition to identify entities in the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Load NER pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample NER results for: Savings is overated\\nIncreasing income is underrat...\n"
     ]
    }
   ],
   "source": [
    "def apply_ner(text, ner_pipe):\n",
    "    \"\"\"Apply NER to identify entities and return entity types and values\"\"\"\n",
    "    if not text or pd.isna(text) or len(text) < 5:\n",
    "        return [], []\n",
    "    \n",
    "    try:\n",
    "        # Limit text length to avoid model limitations\n",
    "        truncated_text = text[:510] if len(text) > 510 else text\n",
    "        \n",
    "        # Apply NER\n",
    "        entities = ner_pipe(truncated_text)\n",
    "        \n",
    "        # Extract entity types and values\n",
    "        entity_types = [e['entity_group'] for e in entities]\n",
    "        entity_values = [e['word'] for e in entities]\n",
    "        \n",
    "        return entity_types, entity_values\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying NER to text: {text[:50]}... - {str(e)}\")\n",
    "        return [], []\n",
    "\n",
    "# Test the NER function on a small sample\n",
    "if 'ner_pipeline' in locals() and not skip_ner and len(df) > 0:\n",
    "    try:\n",
    "        sample_text = df['cleaned_text'].iloc[0]\n",
    "        entity_types, entity_values = apply_ner(sample_text, ner_pipeline)\n",
    "        print(f\"Sample NER results for: {sample_text[:50]}...\")\n",
    "        for t, v in zip(entity_types, entity_values):\n",
    "            print(f\"  - {t}: {v}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing NER function: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying NER to all tweets. This may take a while...\n",
      "Loading NER results from checkpoint file (45875 rows)\n",
      "NER processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Apply NER to all tweets (this may take some time)\n",
    "if not skip_ner and len(df) > 0:\n",
    "    print(\"Applying NER to all tweets. This may take a while...\")\n",
    "\n",
    "    # Check for checkpoint file\n",
    "    checkpoint_file = '../data/ner_checkpoint.csv'\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        try:\n",
    "            checkpoint_df = pd.read_csv(checkpoint_file)\n",
    "            if 'entity_types' in checkpoint_df.columns and 'entity_values' in checkpoint_df.columns:\n",
    "                print(f\"Loading NER results from checkpoint file ({len(checkpoint_df)} rows)\")\n",
    "                df = checkpoint_df\n",
    "                ner_complete = True\n",
    "            else:\n",
    "                ner_complete = False\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {str(e)}\")\n",
    "            ner_complete = False\n",
    "    else:\n",
    "        ner_complete = False\n",
    "\n",
    "    if not ner_complete:\n",
    "        try:\n",
    "            # We'll process in smaller batches to prevent memory issues\n",
    "            batch_size = 100\n",
    "            num_batches = (len(df) + batch_size - 1) // batch_size\n",
    "            \n",
    "            all_entity_types = []\n",
    "            all_entity_values = []\n",
    "            \n",
    "            for i in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, len(df))\n",
    "                \n",
    "                batch_texts = df['cleaned_text'].iloc[start_idx:end_idx].tolist()\n",
    "                batch_results = []\n",
    "                \n",
    "                for text in batch_texts:\n",
    "                    batch_results.append(apply_ner(text, ner_pipeline))\n",
    "                \n",
    "                all_entity_types.extend([r[0] for r in batch_results])\n",
    "                all_entity_values.extend([r[1] for r in batch_results])\n",
    "                \n",
    "                # Save checkpoint every 5 batches\n",
    "                if (i+1) % 5 == 0 or i == num_batches - 1:\n",
    "                    temp_df = df.copy()\n",
    "                    temp_types = all_entity_types + [[] for _ in range(len(df) - len(all_entity_types))]\n",
    "                    temp_values = all_entity_values + [[] for _ in range(len(df) - len(all_entity_values))]\n",
    "                    temp_df['entity_types'] = temp_types\n",
    "                    temp_df['entity_values'] = temp_values\n",
    "                    temp_df.to_csv(checkpoint_file, index=False)\n",
    "                    print(f\"Saved checkpoint after batch {i+1}/{num_batches}\")\n",
    "            \n",
    "            # Add results to dataframe\n",
    "            df['entity_types'] = all_entity_types\n",
    "            df['entity_values'] = all_entity_values\n",
    "            ner_success = True\n",
    "        except Exception as e:\n",
    "            print(f\"Error during NER processing: {str(e)}\")\n",
    "            print(\"Continuing with stock symbol extraction without NER results\")\n",
    "            ner_success = False\n",
    "    else:\n",
    "        ner_success = True\n",
    "\n",
    "    if ner_success:\n",
    "        print(\"NER processing complete!\")\n",
    "        df[['cleaned_text', 'entity_types', 'entity_values']].head()\n",
    "else:\n",
    "    print(\"Skipping NER processing\")\n",
    "    # Initialize empty columns for entity types and values\n",
    "    if len(df) > 0 and 'entity_types' not in df.columns:\n",
    "        df['entity_types'] = [[] for _ in range(len(df))]\n",
    "        df['entity_values'] = [[] for _ in range(len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract and Verify Stock Symbols\n",
    "\n",
    "Extract potential stock symbols from tweets and verify them against actual stock symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_stock_symbols(text):\n",
    "    \"\"\"Extract potential stock symbols from text\"\"\"\n",
    "    if not text or pd.isna(text) or len(text) < 3:\n",
    "        return []\n",
    "    \n",
    "    # Regex patterns for potential stock symbols (1-5 capital letters)\n",
    "    # Looking for patterns like $AAPL, #AAPL, AAPL, or variations surrounded by spaces, punctuation, etc.\n",
    "    stock_patterns = [\n",
    "        r'\\$([A-Z]{1,5})(?![A-Z0-9])',  # $AAPL\n",
    "        r'\\#([A-Z]{1,5})(?![A-Z0-9])',  # #AAPL\n",
    "        r'(?<![A-Z0-9])([A-Z]{1,5})(?![A-Z0-9])'  # AAPL surrounded by non-alphanumeric or space\n",
    "    ]\n",
    "    \n",
    "    matches = []\n",
    "    for pattern in stock_patterns:\n",
    "        symbols = re.findall(pattern, text)\n",
    "        matches.extend(symbols)\n",
    "    \n",
    "    # Filter out common words that might be all caps but aren't stock symbols\n",
    "    common_words = {'A', 'I', 'ME', 'MY', 'THE', 'AND', 'OR', 'AT', 'IN', 'ON', 'BY', 'TO', 'FOR'}\n",
    "    matches = [m for m in matches if m not in common_words]\n",
    "    \n",
    "    # Also consider entities identified as ORG by NER, if available\n",
    "    if 'entity_types' in df.columns and 'entity_values' in df.columns:\n",
    "        # This would need to match the text to a row in the dataframe\n",
    "        # For simplicity, we'll just continue with regex matches for now\n",
    "        pass\n",
    "    \n",
    "    return list(set(matches))  # Return unique matches\n",
    "\n",
    "# Apply to dataframe\n",
    "if len(df) > 0:\n",
    "    df['potential_symbols'] = df['cleaned_text'].apply(extract_stock_symbols)\n",
    "    # Display tweets with potential stock symbols\n",
    "    tweets_with_symbols = df[df['potential_symbols'].apply(len) > 0]\n",
    "    print(f\"Found {len(tweets_with_symbols)} tweets with potential stock symbols\")\n",
    "    df[['cleaned_text', 'potential_symbols']].head(10)\n",
    "else:\n",
    "    print(\"No data to extract stock symbols from\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'potential_symbols'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PIRAS/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'potential_symbols'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m verified \u001b[38;5;28;01mif\u001b[39;00m verified \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Test on a small sample first\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m sample_symbols = df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpotential_symbols\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x) > \u001b[32m0\u001b[39m)][\u001b[33m'\u001b[39m\u001b[33mpotential_symbols\u001b[39m\u001b[33m'\u001b[39m].head(\u001b[32m5\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m symbols \u001b[38;5;129;01min\u001b[39;00m sample_symbols:\n\u001b[32m     36\u001b[39m     verified = get_verified_symbols(symbols)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PIRAS/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/PIRAS/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'potential_symbols'"
     ]
    }
   ],
   "source": [
    "def verify_stock_symbol(symbol, max_retries=3):\n",
    "    \"\"\"Verify if a given string is a valid stock symbol using yfinance with retry logic\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            ticker = yf.Ticker(symbol)\n",
    "            # Try to get some basic info to verify it's valid\n",
    "            info = ticker.info\n",
    "            \n",
    "            # Check if we got valid info (if the ticker doesn't exist, this will be empty or have minimal data)\n",
    "            if 'symbol' in info and info['symbol'] == symbol:\n",
    "                return True\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retry {attempt+1} for {symbol}: {str(e)}\")\n",
    "                time.sleep(1)  # Wait before retrying\n",
    "            else:\n",
    "                print(f\"Failed to verify {symbol} after {max_retries} attempts: {str(e)}\")\n",
    "                return False\n",
    "\n",
    "def get_verified_symbols(symbols_list):\n",
    "    \"\"\"From a list of potential symbols, return the valid ones\"\"\"\n",
    "    if not symbols_list:\n",
    "        return None\n",
    "    \n",
    "    verified = []\n",
    "    for symbol in symbols_list:\n",
    "        if verify_stock_symbol(symbol):\n",
    "            verified.append(symbol)\n",
    "    \n",
    "    return verified if verified else None\n",
    "\n",
    "# Test on a small sample first\n",
    "sample_symbols = df[df['potential_symbols'].apply(lambda x: len(x) > 0)]['potential_symbols'].head(5)\n",
    "for symbols in sample_symbols:\n",
    "    verified = get_verified_symbols(symbols)\n",
    "    print(f\"Potential: {symbols}, Verified: {verified}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply verification to all potential symbols\n",
    "print(\"Verifying stock symbols. This may take a while...\")\n",
    "\n",
    "# Check for checkpoint file\n",
    "symbols_checkpoint_file = '../data/symbols_checkpoint.csv'\n",
    "if os.path.exists(symbols_checkpoint_file):\n",
    "    try:\n",
    "        symbols_df = pd.read_csv(symbols_checkpoint_file)\n",
    "        if 'verified_stock_symbols' in symbols_df.columns:\n",
    "            print(f\"Loading verified symbols from checkpoint file ({len(symbols_df)} rows)\")\n",
    "            df = symbols_df\n",
    "            symbols_complete = True\n",
    "        else:\n",
    "            symbols_complete = False\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading symbols checkpoint: {str(e)}\")\n",
    "        symbols_complete = False\n",
    "else:\n",
    "    symbols_complete = False\n",
    "\n",
    "if not symbols_complete:\n",
    "    # Create a cache to avoid repeated API calls for the same symbol\n",
    "    symbol_cache = {}\n",
    "    \n",
    "    def get_verified_symbols_with_cache(symbols_list):\n",
    "        \"\"\"Use cache to speed up symbol verification\"\"\"\n",
    "        if not symbols_list:\n",
    "            return None\n",
    "        \n",
    "        verified = []\n",
    "        for symbol in symbols_list:\n",
    "            if symbol in symbol_cache:\n",
    "                if symbol_cache[symbol]:\n",
    "                    verified.append(symbol)\n",
    "            else:\n",
    "                is_valid = verify_stock_symbol(symbol)\n",
    "                symbol_cache[symbol] = is_valid\n",
    "                if is_valid:\n",
    "                    verified.append(symbol)\n",
    "        \n",
    "        return verified if verified else None\n",
    "    \n",
    "    # Process in batches and save checkpoints\n",
    "    batch_size = 100\n",
    "    rows_with_symbols = df[df['potential_symbols'].apply(lambda x: len(x) > 0)]\n",
    "    \n",
    "    if len(rows_with_symbols) > 0:\n",
    "        print(f\"Processing {len(rows_with_symbols)} tweets with potential symbols\")\n",
    "        \n",
    "        for i, idx in enumerate(tqdm(rows_with_symbols.index, desc=\"Verifying symbols\")):\n",
    "            symbols = df.loc[idx, 'potential_symbols']\n",
    "            verified = get_verified_symbols_with_cache(symbols)\n",
    "            df.at[idx, 'verified_stock_symbols'] = verified\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if (i+1) % batch_size == 0 or i == len(rows_with_symbols) - 1:\n",
    "                df.to_csv(symbols_checkpoint_file, index=False)\n",
    "                print(f\"Saved checkpoint after processing {i+1}/{len(rows_with_symbols)} rows\")\n",
    "    else:\n",
    "        print(\"No tweets with potential stock symbols found\")\n",
    "\n",
    "df[['cleaned_text', 'potential_symbols', 'verified_stock_symbols']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply verification to all potential symbols\n",
    "print(\"Verifying stock symbols. This may take a while...\")\n",
    "\n",
    "# Create a cache to avoid repeated API calls for the same symbol\n",
    "symbol_cache = {}\n",
    "\n",
    "def get_verified_symbols_with_cache(symbols_list):\n",
    "    \"\"\"Use cache to speed up symbol verification\"\"\"\n",
    "    if not symbols_list:\n",
    "        return None\n",
    "    \n",
    "    verified = []\n",
    "    for symbol in symbols_list:\n",
    "        if symbol in symbol_cache:\n",
    "            if symbol_cache[symbol]:\n",
    "                verified.append(symbol)\n",
    "        else:\n",
    "            is_valid = verify_stock_symbol(symbol)\n",
    "            symbol_cache[symbol] = is_valid\n",
    "            if is_valid:\n",
    "                verified.append(symbol)\n",
    "    \n",
    "    return verified if verified else None\n",
    "\n",
    "# Apply to all rows with potential symbols\n",
    "tqdm.pandas(desc=\"Verifying symbols\")\n",
    "df['verified_stock_symbols'] = df['potential_symbols'].progress_apply(get_verified_symbols_with_cache)\n",
    "df[['cleaned_text', 'potential_symbols', 'verified_stock_symbols']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all processed tweets\n",
    "os.makedirs('../data', exist_ok=True)  # Ensure directory exists\n",
    "df.to_csv('../data/tweets_with_ner_and_stocks.csv', index=False)\n",
    "print(f\"Saved all processed tweets to '../data/tweets_with_ner_and_stocks.csv'\")\n",
    "\n",
    "# Create a filtered dataset with only tweets that mention verified stock symbols\n",
    "if 'verified_stock_symbols' in df.columns:\n",
    "    stock_tweets_df = df[df['verified_stock_symbols'].notnull()].copy()\n",
    "    stock_tweets_df.to_csv('../data/tweets_with_verified_stocks.csv', index=False)\n",
    "    print(f\"Saved {len(stock_tweets_df)} tweets with verified stock symbols to '../data/tweets_with_verified_stocks.csv'\")\n",
    "else:\n",
    "    print(\"Warning: No 'verified_stock_symbols' column found in the dataframe. Check the previous steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data for Labeling\n",
    "\n",
    "Save the processed data with NER results and verified stock symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "After this notebook, you can proceed to the sentiment labeling process using:\n",
    "1. The `tweets_with_verified_stocks.csv` file for tweets that mention specific stocks\n",
    "2. The `tweets_with_ner_and_stocks.csv` file for all tweets with NER information\n",
    "\n",
    "The sentiment labeling should now focus on the tweets with verified stock symbols."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}