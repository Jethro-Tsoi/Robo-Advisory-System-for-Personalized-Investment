{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Tweet Data Preparation\n",
    "\n",
    "This notebook handles the preparation of financial tweets data:\n",
    "1. Loading CSV files\n",
    "2. Data cleaning\n",
    "3. Preparing for Gemini labeling\n",
    "4. Data preprocessing for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Tweet Data\n",
    "\n",
    "Load all CSV files from the data/tweets directory that start with 'row_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_tweet_files(data_dir='../data/tweets/'):\n",
    "    \"\"\"Load all row_*.csv files from the specified directory\"\"\"\n",
    "    all_files = glob(os.path.join(data_dir, 'row_*.csv'))\n",
    "    \n",
    "    dataframes = []\n",
    "    for file in tqdm(all_files, desc='Loading files'):\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            # Add source file name as a column\n",
    "            df['source_file'] = os.path.basename(file)\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {str(e)}\")\n",
    "    \n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Load all tweet data\n",
    "df = load_tweet_files()\n",
    "print(f\"Total tweets loaded: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "Clean the tweet text and remove any unnecessary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def clean_tweet(text):\n",
    "    \"\"\"Basic tweet cleaning function\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove user mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Clean the tweets\n",
    "df['cleaned_text'] = df['text'].apply(clean_tweet)\n",
    "\n",
    "# Remove empty tweets\n",
    "df = df[df['cleaned_text'].str.len() > 0].reset_index(drop=True)\n",
    "print(f\"Tweets after cleaning: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare for Gemini Labeling\n",
    "\n",
    "Create a format suitable for Gemini API labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def prepare_gemini_format(df, batch_size=100):\n",
    "    \"\"\"Prepare data in batches for Gemini API\"\"\"\n",
    "    # Select relevant columns\n",
    "    labeling_df = df[['cleaned_text', 'source_file']].copy()\n",
    "    \n",
    "    # Create batches\n",
    "    num_batches = (len(labeling_df) + batch_size - 1) // batch_size\n",
    "    batches = []\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(labeling_df))\n",
    "        batch = labeling_df.iloc[start_idx:end_idx]\n",
    "        batches.append(batch)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "# Prepare batches for labeling\n",
    "batches = prepare_gemini_format(df)\n",
    "print(f\"Number of batches: {len(batches)}\")\n",
    "\n",
    "# Save first batch as example\n",
    "example_batch = batches[0]\n",
    "example_batch.to_csv('../data/batch_0_for_labeling.csv', index=False)\n",
    "example_batch.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Statistics\n",
    "\n",
    "Calculate some basic statistics about our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_statistics(df):\n",
    "    \"\"\"Calculate and display dataset statistics\"\"\"\n",
    "    stats = {\n",
    "        'Total Tweets': len(df),\n",
    "        'Unique Sources': df['source_file'].nunique(),\n",
    "        'Avg Tweet Length': df['cleaned_text'].str.len().mean(),\n",
    "        'Max Tweet Length': df['cleaned_text'].str.len().max(),\n",
    "        'Min Tweet Length': df['cleaned_text'].str.len().min()\n",
    "    }\n",
    "    \n",
    "    return pd.Series(stats)\n",
    "\n",
    "# Display statistics\n",
    "stats = calculate_statistics(df)\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Use Gemini API to label the batches (will be provided by user)\n",
    "2. Process the labeled data\n",
    "3. Prepare final dataset for model training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
