{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (1.26.0)\n",
      "Requirement already satisfied: requests in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install polars requests tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d595140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "# Removed: # Removed: import google.generativeai as genai\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e855cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyManager:\n",
    "    \"\"\"Manages multiple API keys and rotates them when rate limits are reached\"\"\"\n",
    "    \n",
    "    def __init__(self, env_prefix='MISTRAL_API_KEY'):\n",
    "        \"\"\"\n",
    "        Initialize the key manager\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        env_prefix : str\n",
    "            Prefix for environment variables storing API keys.\n",
    "            Keys should be named like MISTRAL_API_KEY, MISTRAL_API_KEY_1, MISTRAL_API_KEY_2, etc.\n",
    "        \"\"\"\n",
    "        self.env_prefix = env_prefix\n",
    "        self.api_keys = self._load_api_keys()\n",
    "        self.current_index = 0\n",
    "        self.rate_limited_keys = {}  # Track which keys hit rate limits and when they can be used again\n",
    "        self.base_url = \"https://api.mistral.ai/v1/chat/completions\"\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        if not self.api_keys:\n",
    "            print(\"⚠️ No API keys found. Please set at least one API key.\")\n",
    "            key = input(f\"Enter your Mistral AI API key: \")\n",
    "            if key:\n",
    "                self.api_keys.append(key)\n",
    "            else:\n",
    "                raise ValueError(\"No API key provided. Cannot continue.\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.api_keys)} Mistral API keys.\")\n",
    "        \n",
    "    def _load_api_keys(self):\n",
    "        \"\"\"Load API keys from environment variables\"\"\"\n",
    "        api_keys = []\n",
    "        \n",
    "        # Try the base key first\n",
    "        base_key = os.getenv(self.env_prefix)\n",
    "        if base_key:\n",
    "            api_keys.append(base_key)\n",
    "        \n",
    "        # Try numbered keys (MISTRAL_API_KEY_1, MISTRAL_API_KEY_2, etc.)\n",
    "        for i in range(1, 10):  # Check for up to 10 keys\n",
    "            key = os.getenv(f\"{self.env_prefix}_{i}\")\n",
    "            if key:\n",
    "                api_keys.append(key)\n",
    "        \n",
    "        return api_keys\n",
    "    \n",
    "    def get_current_key(self):\n",
    "        \"\"\"Get the current active API key\"\"\"\n",
    "        return self.api_keys[self.current_index]\n",
    "    \n",
    "    def get_current_headers(self):\n",
    "        \"\"\"Get headers with the current API key\"\"\"\n",
    "        headers = self.headers.copy()\n",
    "        headers[\"Authorization\"] = f\"Bearer {self.get_current_key()}\"\n",
    "        return headers\n",
    "    \n",
    "    def rotate_key(self, rate_limited=False, retry_after=60):\n",
    "        \"\"\"\n",
    "        Rotate to the next available API key\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        rate_limited : bool\n",
    "            Whether the current key hit a rate limit\n",
    "        retry_after : int\n",
    "            Seconds until the rate-limited key can be used again\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str : The new API key\n",
    "        \"\"\"\n",
    "        # Mark the current key as rate limited if needed\n",
    "        if rate_limited:\n",
    "            self.rate_limited_keys[self.current_index] = time.time() + retry_after\n",
    "            print(f\"API key {self.current_index + 1} rate limited. Will retry after {retry_after} seconds.\")\n",
    "        \n",
    "        # Try to find a key that's not rate limited\n",
    "        original_index = self.current_index\n",
    "        while True:\n",
    "            self.current_index = (self.current_index + 1) % len(self.api_keys)\n",
    "            \n",
    "            # Check if this key is rate limited\n",
    "            if self.current_index in self.rate_limited_keys:\n",
    "                # Check if enough time has passed\n",
    "                if time.time() > self.rate_limited_keys[self.current_index]:\n",
    "                    # Key is no longer rate limited\n",
    "                    del self.rate_limited_keys[self.current_index]\n",
    "                    break\n",
    "            else:\n",
    "                # Key is not rate limited\n",
    "                break\n",
    "                \n",
    "            # If we've checked all keys and they're all rate limited, use the least recently rate limited one\n",
    "            if self.current_index == original_index:\n",
    "                # Find the key that will be available soonest\n",
    "                soonest_available = min(self.rate_limited_keys.items(), key=lambda x: x[1])\n",
    "                self.current_index = soonest_available[0]\n",
    "                wait_time = max(0, self.rate_limited_keys[self.current_index] - time.time())\n",
    "                \n",
    "                if wait_time > 0:\n",
    "                    print(f\"All API keys are rate limited. Waiting {wait_time:.1f} seconds for the next available key.\")\n",
    "                    time.sleep(wait_time)\n",
    "                    del self.rate_limited_keys[self.current_index]\n",
    "                break\n",
    "        \n",
    "        # Return the new key\n",
    "        key = self.api_keys[self.current_index]\n",
    "        print(f\"Switched to Mistral API key {self.current_index + 1}\")\n",
    "        return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e42b84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 Mistral API keys.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Key Manager\n",
    "key_manager = KeyManager()\n",
    "\n",
    "# Set the model name\n",
    "MODEL = \"mistral-small-latest\"  # Using Mistral's large model instead of Gemini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c9bc61",
   "metadata": {},
   "source": [
    "\n",
    "> **⚠️ API Key Setup**\n",
    ">\n",
    "> This notebook uses the Mistral AI API with support for multiple API keys:\n",
    ">\n",
    "> 1. Set your primary API key as `MISTRAL_API_KEY` environment variable\n",
    "> 2. For additional keys, use `MISTRAL_API_KEY_1`, `MISTRAL_API_KEY_2`, etc.\n",
    "> 3. The system will automatically rotate between keys if rate limits are encountered\n",
    ">\n",
    "> Keys can be created at [Mistral AI Platform](https://console.mistral.ai/)\n",
    ">\n",
    "> **Mistral AI Free Tier Limits:**\n",
    "> - 1 request per second (60 requests per minute)\n",
    "> - 500,000 tokens per minute\n",
    "> - 1 billion tokens per month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "700448bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_prompt(text):\n",
    "    \"\"\"Configure the prompt for Mistral\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "            You are a financial sentiment analyzer. Classify the given tweet's sentiment into one of these categories:\n",
    "\n",
    "            STRONGLY_POSITIVE - Very bullish, highly confident optimistic outlook\n",
    "            POSITIVE - Generally optimistic, bullish view\n",
    "            NEUTRAL - Factual, balanced, or no clear sentiment\n",
    "            NEGATIVE - Generally pessimistic, bearish view\n",
    "            STRONGLY_NEGATIVE - Very bearish, highly confident pessimistic outlook\n",
    "\n",
    "            Examples:\n",
    "            \"Breaking: Company XYZ doubles profit forecast!\" -> STRONGLY_POSITIVE\n",
    "            \"Expecting modest gains next quarter\" -> POSITIVE\n",
    "            \"Market closed at 35,000\" -> NEUTRAL\n",
    "            \"Concerned about rising rates\" -> NEGATIVE\n",
    "            \"Crash incoming, sell everything!\" -> STRONGLY_NEGATIVE\n",
    "\n",
    "            Format: Return only one word from: STRONGLY_POSITIVE, POSITIVE, NEUTRAL, NEGATIVE, STRONGLY_NEGATIVE\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Analyze the sentiment of this tweet: {text}\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbf82b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text, retries=3):\n",
    "    \"\"\"Get sentiment from Mistral AI with retry logic\"\"\"\n",
    "    if not text or len(str(text).strip()) < 3:\n",
    "        return 'NEUTRAL'\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # Setup the API request for Mistral\n",
    "            headers = key_manager.get_current_headers()\n",
    "            payload = {\n",
    "                \"model\": MODEL,\n",
    "                \"temperature\": 0.0,  # Deterministic output\n",
    "                \"max_tokens\": 10,    # We only need one word\n",
    "                \"messages\": setup_prompt(text)\n",
    "            }\n",
    "            \n",
    "            # Make the API request\n",
    "            response = requests.post(\n",
    "                key_manager.base_url,\n",
    "                headers=headers,\n",
    "                json=payload\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Extract sentiment from Mistral's response\n",
    "                response_json = response.json()\n",
    "                sentiment = response_json['choices'][0]['message']['content'].strip().upper()\n",
    "                \n",
    "                # Validate the response\n",
    "                valid_labels = [\n",
    "                    'STRONGLY_POSITIVE', 'POSITIVE', 'NEUTRAL', 'NEGATIVE', 'STRONGLY_NEGATIVE'\n",
    "                ]\n",
    "                \n",
    "                if sentiment in valid_labels:\n",
    "                    return sentiment\n",
    "                else:\n",
    "                    print(f\"Invalid sentiment received: {sentiment}, defaulting to NEUTRAL\")\n",
    "                    return 'NEUTRAL'\n",
    "            elif response.status_code == 429:  # Rate limit\n",
    "                # Get retry_after time if provided\n",
    "                retry_after = int(response.headers.get('Retry-After', 5))\n",
    "                key_manager.rotate_key(rate_limited=True, retry_after=retry_after)\n",
    "                if attempt < retries - 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    return 'NEUTRAL'\n",
    "            else:\n",
    "                print(f\"API error: {response.status_code} - {response.text}\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(2)  # Wait before retry\n",
    "                    continue\n",
    "                else:\n",
    "                    return 'NEUTRAL'\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_str = str(e).lower()\n",
    "            # Check for rate limiting errors\n",
    "            if \"quota\" in error_str or \"rate\" in error_str or \"429\" in error_str:\n",
    "                # Extract retry time if available (default to 5 seconds if not found)\n",
    "                retry_after = 0\n",
    "                if \"retryafter\" in error_str or \"retry-after\" in error_str or \"retry_after\" in error_str:\n",
    "                    try:\n",
    "                        # Try to extract the retry time\n",
    "                        matches = re.findall(r'retry.*?(\\\\d+)', error_str)\n",
    "                        if matches:\n",
    "                            retry_after = int(matches[0])\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                # Switch to another API key if there are multiple keys\n",
    "                if len(key_manager.api_keys) > 1:\n",
    "                    key_manager.rotate_key(rate_limited=True, retry_after=retry_after)\n",
    "                    if attempt < retries - 1:\n",
    "                        continue\n",
    "                else:\n",
    "                    # Only one key, just wait\n",
    "                    wait_time = min(2 ** attempt * 5, retry_after)  # Exponential backoff with max retry_after\n",
    "                    print(f\"Rate limit hit - waiting {wait_time}s before retry ({attempt+1}/{retries})\")\n",
    "                    time.sleep(wait_time)\n",
    "                    if attempt < retries - 1:\n",
    "                        continue\n",
    "                    \n",
    "            if attempt == retries - 1:\n",
    "                print(f\"Error processing text: {str(text)[:50]}...\\nError: {str(e)}\")\n",
    "                return 'NEUTRAL'\n",
    "            time.sleep(2)  # Wait before retry\n",
    "    \n",
    "    return 'NEUTRAL'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc0b913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test tweet: 'Breaking: Tesla stock hits all-time high after unexpected profit surge'\n",
      "Sentiment: STRONGLY_POSITIVE\n",
      "Using Mistral API key index: 0\n"
     ]
    }
   ],
   "source": [
    "# Test the sentiment analysis with key rotation\n",
    "test_tweet = \"Breaking: Tesla stock hits all-time high after unexpected profit surge\"\n",
    "sentiment = get_sentiment(test_tweet)\n",
    "print(f\"Test tweet: '{test_tweet}'\")\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Using Mistral API key index: {key_manager.current_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stock market tweets dataset from Hugging Face...\n",
      "Loaded 28172 tweets\n",
      "\n",
      "Sample tweets:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>image_url</th><th>proxy_image_url</th><th>image_dimensions</th><th>thumbnail_url</th><th>proxy_thumbnail_url</th><th>thumbnail_dimensions</th><th>timestamp</th><th>description</th><th>url</th><th>embed_title</th><th>tweet_type</th><th>financial_info</th><th>sentiment</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;https://pbs.twimg.com/media/F-…</td><td>&quot;https://images-ext-1.discordap…</td><td>&quot;(749, 1085)&quot;</td><td>&quot;https://pbs.twimg.com/profile_…</td><td>&quot;https://images-ext-1.discordap…</td><td>&quot;(48, 48)&quot;</td><td>&quot;2023-11-15T01:06:39.739000+00:…</td><td>&quot;Currently at $2860--Documentin…</td><td>&quot;https://twitter.com/user/statu…</td><td>&quot;&lt;:quote_tweet:1130467736133316…</td><td>&quot;quote tweet&quot;</td><td>&quot;[{&#x27;ticker&#x27;: &#x27;$AI&#x27;, &#x27;exchanges&#x27;…</td><td>&quot;Bullish&quot;</td></tr><tr><td>&quot;https://pbs.twimg.com/media/F-…</td><td>&quot;https://images-ext-1.discordap…</td><td>&quot;(653, 1199)&quot;</td><td>&quot;https://pbs.twimg.com/profile_…</td><td>&quot;https://images-ext-1.discordap…</td><td>&quot;(48, 48)&quot;</td><td>&quot;2023-11-15T02:11:41.182000+00:…</td><td>&quot;$NET \n",
       "\n",
       "Was waiting on Cloudfla…</td><td>&quot;https://twitter.com/user/statu…</td><td>&quot;Don&#x27;t follow Shardi B If You H…</td><td>&quot;tweet&quot;</td><td>&quot;[{&#x27;ticker&#x27;: &#x27;$NET&#x27;, &#x27;exchanges…</td><td>&quot;Neutral&quot;</td></tr><tr><td>&quot;https://pbs.twimg.com/media/F-…</td><td>&quot;https://images-ext-1.discordap…</td><td>&quot;(1200, 591)&quot;</td><td>&quot;https://pbs.twimg.com/profile_…</td><td>&quot;https://images-ext-1.discordap…</td><td>&quot;(48, 48)&quot;</td><td>&quot;2023-11-15T02:11:47.180000+00:…</td><td>&quot;RT @TrendSpider: $SPY, $QQQ an…</td><td>&quot;https://twitter.com/user/statu…</td><td>&quot;&lt;:retweet:1130467740306657360&gt;…</td><td>&quot;retweet&quot;</td><td>&quot;[{&#x27;ticker&#x27;: &#x27;$QQQ&#x27;, &#x27;exchanges…</td><td>&quot;Bearish&quot;</td></tr><tr><td>&quot;https://pbs.twimg.com/media/F-…</td><td>&quot;https://images-ext-1.discordap…</td><td>&quot;(1200, 653)&quot;</td><td>&quot;https://pbs.twimg.com/profile_…</td><td>&quot;https://images-ext-1.discordap…</td><td>&quot;(48, 48)&quot;</td><td>&quot;2023-11-15T04:01:40.263000+00:…</td><td>&quot;RT @TriggerTrades: $SPX remain…</td><td>&quot;https://twitter.com/user/statu…</td><td>&quot;&lt;:retweet:1130467740306657360&gt;…</td><td>&quot;retweet&quot;</td><td>&quot;[{&#x27;ticker&#x27;: &#x27;$SPX&#x27;, &#x27;exchanges…</td><td>&quot;Bearish&quot;</td></tr><tr><td>&quot;https://pbs.twimg.com/media/F-…</td><td>&quot;https://images-ext-1.discordap…</td><td>&quot;(884, 665)&quot;</td><td>&quot;https://pbs.twimg.com/profile_…</td><td>&quot;https://images-ext-1.discordap…</td><td>&quot;(48, 48)&quot;</td><td>&quot;2023-11-15T04:01:44.817000+00:…</td><td>&quot;RT @coiledspringcap: #SPX equa…</td><td>&quot;https://twitter.com/user/statu…</td><td>&quot;&lt;:retweet:1130467740306657360&gt;…</td><td>&quot;retweet&quot;</td><td>&quot;[{&#x27;ticker&#x27;: &#x27;$SPX&#x27;, &#x27;exchanges…</td><td>&quot;Bullish&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 13)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ image_url ┆ proxy_ima ┆ image_dim ┆ thumbnail ┆ … ┆ embed_tit ┆ tweet_typ ┆ financial ┆ sentimen │\n",
       "│ ---       ┆ ge_url    ┆ ensions   ┆ _url      ┆   ┆ le        ┆ e         ┆ _info     ┆ t        │\n",
       "│ str       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│           ┆ str       ┆ str       ┆ str       ┆   ┆ str       ┆ str       ┆ str       ┆ str      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ https://p ┆ https://i ┆ (749,     ┆ https://p ┆ … ┆ <:quote_t ┆ quote     ┆ [{'ticker ┆ Bullish  │\n",
       "│ bs.twimg. ┆ mages-ext ┆ 1085)     ┆ bs.twimg. ┆   ┆ weet:1130 ┆ tweet     ┆ ': '$AI', ┆          │\n",
       "│ com/media ┆ -1.discor ┆           ┆ com/profi ┆   ┆ 467736133 ┆           ┆ 'exchange ┆          │\n",
       "│ /F-…      ┆ dap…      ┆           ┆ le_…      ┆   ┆ 316…      ┆           ┆ s'…       ┆          │\n",
       "│ https://p ┆ https://i ┆ (653,     ┆ https://p ┆ … ┆ Don't     ┆ tweet     ┆ [{'ticker ┆ Neutral  │\n",
       "│ bs.twimg. ┆ mages-ext ┆ 1199)     ┆ bs.twimg. ┆   ┆ follow    ┆           ┆ ':        ┆          │\n",
       "│ com/media ┆ -1.discor ┆           ┆ com/profi ┆   ┆ Shardi B  ┆           ┆ '$NET',   ┆          │\n",
       "│ /F-…      ┆ dap…      ┆           ┆ le_…      ┆   ┆ If You H… ┆           ┆ 'exchange ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ s…        ┆          │\n",
       "│ https://p ┆ https://i ┆ (1200,    ┆ https://p ┆ … ┆ <:retweet ┆ retweet   ┆ [{'ticker ┆ Bearish  │\n",
       "│ bs.twimg. ┆ mages-ext ┆ 591)      ┆ bs.twimg. ┆   ┆ :11304677 ┆           ┆ ':        ┆          │\n",
       "│ com/media ┆ -1.discor ┆           ┆ com/profi ┆   ┆ 403066573 ┆           ┆ '$QQQ',   ┆          │\n",
       "│ /F-…      ┆ dap…      ┆           ┆ le_…      ┆   ┆ 60>…      ┆           ┆ 'exchange ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ s…        ┆          │\n",
       "│ https://p ┆ https://i ┆ (1200,    ┆ https://p ┆ … ┆ <:retweet ┆ retweet   ┆ [{'ticker ┆ Bearish  │\n",
       "│ bs.twimg. ┆ mages-ext ┆ 653)      ┆ bs.twimg. ┆   ┆ :11304677 ┆           ┆ ':        ┆          │\n",
       "│ com/media ┆ -1.discor ┆           ┆ com/profi ┆   ┆ 403066573 ┆           ┆ '$SPX',   ┆          │\n",
       "│ /F-…      ┆ dap…      ┆           ┆ le_…      ┆   ┆ 60>…      ┆           ┆ 'exchange ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ s…        ┆          │\n",
       "│ https://p ┆ https://i ┆ (884,     ┆ https://p ┆ … ┆ <:retweet ┆ retweet   ┆ [{'ticker ┆ Bullish  │\n",
       "│ bs.twimg. ┆ mages-ext ┆ 665)      ┆ bs.twimg. ┆   ┆ :11304677 ┆           ┆ ':        ┆          │\n",
       "│ com/media ┆ -1.discor ┆           ┆ com/profi ┆   ┆ 403066573 ┆           ┆ '$SPX',   ┆          │\n",
       "│ /F-…      ┆ dap…      ┆           ┆ le_…      ┆   ┆ 60>…      ┆           ┆ 'exchange ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ s…        ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from Hugging Face\n",
    "print(\"Loading stock market tweets dataset from Hugging Face...\")\n",
    "\n",
    "# Check if the huggingface datasets library is installed\n",
    "try:\n",
    "    import huggingface_hub\n",
    "except ImportError:\n",
    "    print(\"Installing huggingface_hub...\")\n",
    "    !pip install huggingface_hub\n",
    "    import huggingface_hub\n",
    "\n",
    "# Load the dataset using Polars\n",
    "df = pl.read_csv('hf://datasets/StephanAkkerman/financial-tweets-stocks/stock.csv')\n",
    "\n",
    "\n",
    "print(f\"Loaded {df.shape[0]} tweets\")\n",
    "print(\"\\nSample tweets:\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'description' column for tweet text\n",
      "\n",
      "Analyzing sentiment for 28172 tweets\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset for sentiment analysis\n",
    "# Let's make sure we have the 'body' column which contains the tweet text\n",
    "if 'description' in df.columns:\n",
    "    tweet_column = 'description'\n",
    "# elif 'full_text' in df.columns:\n",
    "#     tweet_column = 'full_text'\n",
    "else:\n",
    "    raise ValueError(\"Could not find tweet text column in the dataset\")\n",
    "\n",
    "print(f\"Using '{tweet_column}' column for tweet text\")\n",
    "\n",
    "# For demonstration, let's use a small subset of the data\n",
    "# Use all data instead of a sample - WARNING: This will process 1.7M tweets!\n",
    "sample_size = df.shape[0]  # Adjust based on your needs\n",
    "sample_df = df.sample(sample_size, seed=42)\n",
    "\n",
    "print(f\"\\nAnalyzing sentiment for {sample_size} tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb1756",
   "metadata": {},
   "source": [
    "\n",
    "## Rate Limits for Mistral AI API\n",
    "\n",
    "Mistral AI's free tier has the following limits:\n",
    "- 1 request per second (60 requests per minute)\n",
    "- 500,000 tokens per minute\n",
    "- 1 billion tokens per month\n",
    "\n",
    "This notebook implements:\n",
    "1. Key rotation to handle multiple API keys\n",
    "2. Automatic retry with exponential backoff\n",
    "3. Batch processing to optimize throughput\n",
    "4. Error handling to ensure robust processing\n",
    "\n",
    "If you need to process many tweets, consider:\n",
    "- Creating multiple API keys\n",
    "- Adjusting batch size and workers based on your needs\n",
    "- Processing tweets in smaller batches with appropriate delays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0a0cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweets, batch_size=4, max_workers=2):\n",
    "    \"\"\"Process tweets in batches with Mistral AI API\n",
    "    \n",
    "    Mistral AI free tier allows:\n",
    "    - 1 request per second (60 requests per minute)\n",
    "    - 500,000 tokens per minute\n",
    "    - 1 billion tokens per month\n",
    "    \"\"\"\n",
    "    all_sentiments = []\n",
    "    \n",
    "    # For demonstration, we'll process a reasonable number of tweets\n",
    "    # Adjust max_tweets if needed - 200 is safe for the free tier\n",
    "    max_tweets = min(200, len(tweets))\n",
    "    tweets = tweets[:max_tweets]\n",
    "    \n",
    "    print(f\"Processing {max_tweets} tweets using Mistral AI API with {len(key_manager.api_keys)} API keys\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for i in tqdm(range(0, len(tweets), batch_size), desc=\"Processing tweet batches\"):\n",
    "            batch = tweets[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                # Process tweets in parallel\n",
    "                results = list(executor.map(get_sentiment, batch))\n",
    "                all_sentiments.extend(results)\n",
    "                \n",
    "                # Add a short delay between batches to avoid rate limiting\n",
    "                time.sleep(2)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "                # Add neutral sentiments for this batch in case of failure\n",
    "                all_sentiments.extend(['NEUTRAL'] * len(batch))\n",
    "                time.sleep(5)  # Wait a bit longer after an error\n",
    "                \n",
    "    # If we didn't get enough sentiments (due to errors), fill with NEUTRAL\n",
    "    if len(all_sentiments) < len(tweets):\n",
    "        all_sentiments.extend(['NEUTRAL'] * (len(tweets) - len(all_sentiments)))\n",
    "            \n",
    "    return all_sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48ef2b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test tweet: 'Breaking: Tesla stock hits all-time high after unexpected profit surge'\n",
      "Sentiment: STRONGLY_POSITIVE\n",
      "Using API key index: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the sentiment analysis with key rotation\n",
    "test_tweet = \"Breaking: Tesla stock hits all-time high after unexpected profit surge\"\n",
    "sentiment = get_sentiment(test_tweet)\n",
    "print(f\"Test tweet: '{test_tweet}'\")\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Using API key index: {key_manager.current_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b23691ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweets, batch_size=4, max_workers=2):\n",
    "    \"\"\"Process tweets in batches with Mistral AI API\n",
    "    \n",
    "    Mistral AI free tier allows:\n",
    "    - 1 request per second (60 requests per minute)\n",
    "    - 500,000 tokens per minute\n",
    "    - 1 billion tokens per month\n",
    "    \"\"\"\n",
    "    all_sentiments = []\n",
    "    \n",
    "    # Process all tweets\n",
    "    # Removed limitation: max_tweets = min(200, len(tweets))\n",
    "    # Removed limitation: tweets = tweets[:max_tweets]\n",
    "    \n",
    "    print(f\"Processing {len(tweets)} tweets using Mistral AI API with {len(key_manager.api_keys)} API keys\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for i in tqdm(range(0, len(tweets), batch_size), desc=\"Processing tweet batches\"):\n",
    "            batch = tweets[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                # Process tweets in parallel\n",
    "                results = list(executor.map(get_sentiment, batch))\n",
    "                all_sentiments.extend(results)\n",
    "                \n",
    "                # Add a delay between batches to respect Mistral's 1 req/sec rate limit\n",
    "                # With batch_size and max_workers, adjust sleep time accordingly\n",
    "                time.sleep(batch_size * max_workers)  # Conservative approach\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "                # Add neutral sentiments for this batch in case of failure\n",
    "                all_sentiments.extend(['NEUTRAL'] * len(batch))\n",
    "                time.sleep(5)  # Wait a bit longer after an error\n",
    "                \n",
    "    # If we didn't get enough sentiments (due to errors), fill with NEUTRAL\n",
    "    if len(all_sentiments) < len(tweets):\n",
    "        all_sentiments.extend(['NEUTRAL'] * (len(tweets) - len(all_sentiments)))\n",
    "            \n",
    "    return all_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "concurrent_function_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets_concurrent_resumed(tweets, start_batch=0, batch_size=4, max_workers=2, save_every=100, rate_limit_wait=5, force_reprocess=False):\n",
    "    \"\"\"Process tweets in batches with Mistral AI API using concurrent API keys, resuming from a specific batch\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tweets : list\n",
    "        List of tweets to process\n",
    "    start_batch : int\n",
    "        Batch index to start processing from (for resuming interrupted processing)\n",
    "    batch_size : int\n",
    "        Number of tweets to process in each batch\n",
    "    max_workers : int\n",
    "        Number of concurrent workers\n",
    "    save_every : int\n",
    "        Save results after processing this many tweets\n",
    "    rate_limit_wait : int\n",
    "        Default wait time in seconds when rate limited (default: 5)\n",
    "    force_reprocess : bool\n",
    "        If True, will reprocess tweets even if they already have sentiment values\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list : Sentiment labels for each tweet\n",
    "    \"\"\"\n",
    "    # Initialize the global key manager if it doesn't exist\n",
    "    global key_manager\n",
    "    if 'key_manager' not in globals():\n",
    "        key_manager = KeyManager()\n",
    "    \n",
    "    # Set model name if not defined\n",
    "    global MODEL\n",
    "    if 'MODEL' not in globals():\n",
    "        MODEL = \"mistral-small-latest\"\n",
    "    \n",
    "    # Load partial results if available\n",
    "    save_path = \"../data/labeled_stock_tweets_partial.csv\"\n",
    "    try:\n",
    "        partial_df = pl.read_csv(save_path)\n",
    "        # Extract existing sentiments from the partial results\n",
    "        all_sentiments = partial_df['sentiment'].to_list()\n",
    "        print(f\"Loaded {len(all_sentiments)} existing sentiment labels\")\n",
    "    except:\n",
    "        # If no partial results exist, pre-allocate the array with None values\n",
    "        all_sentiments = [None] * len(tweets)\n",
    "        print(\"No existing results found, starting fresh\")\n",
    "    \n",
    "    # Sort tweet indices in descending order (process larger row numbers first)\n",
    "    tweet_indices = list(range(len(tweets)))\n",
    "    tweet_indices.sort(reverse=True)\n",
    "    \n",
    "    # Create a KeyManager for each worker\n",
    "    key_managers = []\n",
    "    \n",
    "    # Check how many API keys we have available\n",
    "    available_keys = key_manager.api_keys.copy()\n",
    "    num_keys = len(available_keys)\n",
    "    \n",
    "    print(f\"Using {num_keys} API keys concurrently for processing {len(tweets)} tweets\")\n",
    "    print(f\"Resuming from batch {start_batch}\")\n",
    "    \n",
    "    if num_keys == 0:\n",
    "        raise ValueError(\"No API keys available\")\n",
    "    \n",
    "    # Create a manager for each key\n",
    "    for i, key in enumerate(available_keys):\n",
    "        # Create a separate manager for each key\n",
    "        km = KeyManager()\n",
    "        # Replace the API keys with just one key\n",
    "        km.api_keys = [key]\n",
    "        km.current_index = 0\n",
    "        key_managers.append(km)\n",
    "    \n",
    "    # If force_reprocess is True, clear the sentiments for batches we want to reprocess\n",
    "    if force_reprocess and start_batch > 0:\n",
    "        # Create batches first to identify which indices to clear\n",
    "        temp_batches = []\n",
    "        for i in range(0, len(tweet_indices), batch_size):\n",
    "            batch_indices = tweet_indices[i:i+batch_size]\n",
    "            temp_batches.append(batch_indices)\n",
    "        \n",
    "        # Clear sentiments for the batches we want to reprocess\n",
    "        for batch_idx in range(start_batch, len(temp_batches)):\n",
    "            for idx in temp_batches[batch_idx]:\n",
    "                if idx < len(all_sentiments):\n",
    "                    all_sentiments[idx] = None\n",
    "        \n",
    "        print(f\"Cleared sentiments for batches starting from {start_batch} to force reprocessing\")\n",
    "    \n",
    "    # Function to process a tweet with a specific key manager\n",
    "    def process_tweet_with_key(args):\n",
    "        idx, tweet, key_idx = args\n",
    "        # Skip already processed tweets (with non-None sentiments)\n",
    "        if not force_reprocess and idx < len(all_sentiments) and all_sentiments[idx] is not None:\n",
    "            return (idx, all_sentiments[idx])\n",
    "        \n",
    "        # Get the key manager for this worker\n",
    "        km = key_managers[key_idx % num_keys]\n",
    "        \n",
    "        # Define a local get_sentiment function that uses this specific key manager\n",
    "        def local_get_sentiment(text, retries=3):\n",
    "            if not text or len(str(text).strip()) < 3:\n",
    "                return 'NEUTRAL'\n",
    "            \n",
    "            for attempt in range(retries):\n",
    "                try:\n",
    "                    # Setup the API request for Mistral\n",
    "                    headers = km.get_current_headers()\n",
    "                    payload = {\n",
    "                        \"model\": MODEL,\n",
    "                        \"temperature\": 0.0,  # Deterministic output\n",
    "                        \"max_tokens\": 10,    # We only need one word\n",
    "                        \"messages\": setup_prompt(text)\n",
    "                    }\n",
    "                    \n",
    "                    # Make the API request\n",
    "                    response = requests.post(\n",
    "                        km.base_url,\n",
    "                        headers=headers,\n",
    "                        json=payload\n",
    "                    )\n",
    "                    \n",
    "                    if response.status_code == 200:\n",
    "                        # Extract sentiment from Mistral's response\n",
    "                        response_json = response.json()\n",
    "                        sentiment = response_json['choices'][0]['message']['content'].strip().upper()\n",
    "                        \n",
    "                        # Validate the response\n",
    "                        valid_labels = [\n",
    "                            'STRONGLY_POSITIVE', 'POSITIVE', 'NEUTRAL', 'NEGATIVE', 'STRONGLY_NEGATIVE'\n",
    "                        ]\n",
    "                        \n",
    "                        if sentiment in valid_labels:\n",
    "                            return sentiment\n",
    "                        else:\n",
    "                            print(f\"Invalid sentiment received: {sentiment}, defaulting to NEUTRAL\")\n",
    "                            return 'NEUTRAL'\n",
    "                    elif response.status_code == 429:  # Rate limit\n",
    "                        # If rate limited, just wait instead of switching keys\n",
    "                        # Always use rate_limit_wait as fallback\n",
    "                        retry_after = int(response.headers.get('Retry-After', rate_limit_wait))\n",
    "                        print(f\"API key {key_idx+1} rate limited. Waiting {retry_after} seconds.\")\n",
    "                        time.sleep(retry_after)\n",
    "                        if attempt < retries - 1:\n",
    "                            continue\n",
    "                        else:\n",
    "                            return 'NEUTRAL'\n",
    "                    else:\n",
    "                        print(f\"API error: {response.status_code} - {response.text}\")\n",
    "                        if attempt < retries - 1:\n",
    "                            time.sleep(2)  # Wait before retry\n",
    "                            continue\n",
    "                        else:\n",
    "                            return 'NEUTRAL'\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    error_str = str(e).lower()\n",
    "                    # Check for rate limiting errors\n",
    "                    if \"quota\" in error_str or \"rate\" in error_str or \"429\" in error_str:\n",
    "                        # Extract retry time if available (default to rate_limit_wait if not found)\n",
    "                        retry_after = rate_limit_wait\n",
    "                        if \"retryafter\" in error_str or \"retry-after\" in error_str or \"retry_after\" in error_str:\n",
    "                            try:\n",
    "                                # Try to extract the retry time\n",
    "                                matches = re.findall(r'retry.*?(\\d+)', error_str)\n",
    "                                if matches:\n",
    "                                    retry_after = int(matches[0])\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                        # Just wait instead of switching keys\n",
    "                        wait_time = min(2 ** attempt * 2, retry_after)  # Use smaller exponential backoff\n",
    "                        print(f\"Rate limit hit for key {key_idx+1} - waiting {wait_time}s before retry ({attempt+1}/{retries})\")\n",
    "                        time.sleep(wait_time)\n",
    "                        if attempt < retries - 1:\n",
    "                            continue\n",
    "                            \n",
    "                    if attempt == retries - 1:\n",
    "                        print(f\"Error processing text: {str(text)[:50]}...\\nError: {str(e)}\")\n",
    "                        return 'NEUTRAL'\n",
    "                    time.sleep(2)  # Wait before retry\n",
    "            \n",
    "            return 'NEUTRAL'\n",
    "        \n",
    "        # Process the tweet\n",
    "        try:\n",
    "            result = local_get_sentiment(tweet)\n",
    "            return (idx, result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tweet {idx}: {e}\")\n",
    "            return (idx, 'NEUTRAL')\n",
    "    \n",
    "    # Create batches for processing\n",
    "    batches = []\n",
    "    for i in range(0, len(tweet_indices), batch_size):\n",
    "        batch_indices = tweet_indices[i:i+batch_size]\n",
    "        # More balanced key assignment - round robin style\n",
    "        batch = [(idx, tweets[idx], (i + idx) % num_keys) for idx in batch_indices]\n",
    "        batches.append(batch)\n",
    "    \n",
    "    # Process batches and periodically save results\n",
    "    # Calculate the processed count based on non-None values in all_sentiments\n",
    "    processed_count = sum(1 for s in all_sentiments if s is not None)\n",
    "    print(f\"Found {processed_count} previously processed tweets\")\n",
    "    \n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    # Skip to the starting batch\n",
    "    batches = batches[start_batch:]\n",
    "    print(f\"Skipping {start_batch} batches, {start_batch * batch_size} tweets\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for batch_idx, batch in enumerate(tqdm(batches, desc=f\"Processing tweet batches (starting from batch {start_batch})\")):\n",
    "            try:\n",
    "                # Process batch\n",
    "                results = list(executor.map(process_tweet_with_key, batch))\n",
    "                \n",
    "                # Update results\n",
    "                for idx, sentiment in results:\n",
    "                    # Only update if it wasn't already set (avoiding repeat API calls)\n",
    "                    if idx < len(all_sentiments) and (all_sentiments[idx] is None or force_reprocess):\n",
    "                        all_sentiments[idx] = sentiment\n",
    "                        processed_count += 1\n",
    "                \n",
    "                # Periodically save results\n",
    "                if (processed_count % save_every < batch_size) or (batch_idx % 50 == 0 and batch_idx > 0):\n",
    "                    # Create a temporary dataframe with current results\n",
    "                    try:\n",
    "                        # Try to load the original dataframe\n",
    "                        global df\n",
    "                        if 'df' not in globals():\n",
    "                            # If df is not defined globally, create a stub\n",
    "                            # In actual use, you would need to make sure df is defined\n",
    "                            # before calling this function\n",
    "                            raise NameError(\"df not defined\")\n",
    "                        \n",
    "                        temp_df = df.clone()\n",
    "                        # Only include processed tweets (non-None sentiments)\n",
    "                        valid_sentiments = [s if s is not None else 'NEUTRAL' for s in all_sentiments]\n",
    "                        temp_df = temp_df.with_columns(pl.Series(name='sentiment', values=valid_sentiments))\n",
    "                        # Save to CSV\n",
    "                        temp_df.write_csv(save_path)\n",
    "                        print(f\"\\nSaved {processed_count} processed tweets to {save_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error saving results: {e}\")\n",
    "                    \n",
    "                    # Save progress information to a JSON file\n",
    "                    import json\n",
    "                    progress = {\n",
    "                        \"current_batch\": start_batch + batch_idx + 1,\n",
    "                        \"processed_count\": processed_count,\n",
    "                        \"total_batches\": len(batches) + start_batch,\n",
    "                        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    }\n",
    "                    with open(\"../data/labeled_stock_tweets_progress.json\", \"w\") as f:\n",
    "                        json.dump(progress, f)\n",
    "                \n",
    "                # Add a short delay between batches to avoid API overload\n",
    "                # Uncomment if needed\n",
    "                # time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "                # Continue with the next batch\n",
    "                time.sleep(5)  # Wait a bit longer after an error\n",
    "    \n",
    "    # Replace any None values with NEUTRAL\n",
    "    all_sentiments = [s if s is not None else 'NEUTRAL' for s in all_sentiments]\n",
    "    \n",
    "    return all_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10544ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28172 existing sentiment labels\n",
      "Using 2 API keys concurrently for processing 28172 tweets\n",
      "Resuming from batch 1500\n",
      "Loaded 2 Mistral API keys.\n",
      "Loaded 2 Mistral API keys.\n",
      "Found 28172 previously processed tweets\n",
      "Skipping 1500 batches, 6000 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweet batches (starting from batch 1500):   0%|          | 0/5543 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweet batches (starting from batch 1500):   7%|▋         | 401/5543 [00:00<00:01, 3820.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweet batches (starting from batch 1500):  16%|█▋        | 901/5543 [00:00<00:01, 4391.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweet batches (starting from batch 1500):  24%|██▍       | 1351/5543 [00:00<00:00, 4326.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweet batches (starting from batch 1500):  32%|███▏      | 1801/5543 [00:00<00:00, 4240.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweet batches (starting from batch 1500):  47%|████▋     | 2601/5543 [00:00<00:00, 3127.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweet batches (starting from batch 1500):  66%|██████▌   | 3651/5543 [00:00<00:00, 4048.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweet batches (starting from batch 1500):  74%|███████▍  | 4101/5543 [00:01<00:00, 3140.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweet batches (starting from batch 1500):  81%|████████  | 4501/5543 [00:01<00:00, 3293.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweet batches (starting from batch 1500):  90%|█████████ | 5001/5543 [00:01<00:00, 3706.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweet batches (starting from batch 1500): 100%|██████████| 5543/5543 [00:01<00:00, 3721.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n",
      "\n",
      "Saved 28172 processed tweets to ../data/labeled_stock_tweets_partial.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>description</th><th>sentiment</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;Currently at $2860--Documentin…</td><td>&quot;NEUTRAL&quot;</td></tr><tr><td>&quot;$NET \n",
       "\n",
       "Was waiting on Cloudfla…</td><td>&quot;NEUTRAL&quot;</td></tr><tr><td>&quot;RT @TrendSpider: $SPY, $QQQ an…</td><td>&quot;NEUTRAL&quot;</td></tr><tr><td>&quot;RT @TriggerTrades: $SPX remain…</td><td>&quot;NEUTRAL&quot;</td></tr><tr><td>&quot;RT @coiledspringcap: #SPX equa…</td><td>&quot;NEUTRAL&quot;</td></tr><tr><td>&quot;RT @coiledspringcap: Everyone …</td><td>&quot;NEUTRAL&quot;</td></tr><tr><td>&quot;RT @SmartReversals: $IWM - Dai…</td><td>&quot;NEUTRAL&quot;</td></tr><tr><td>&quot;RT @SmartReversals: $NDX - Dai…</td><td>&quot;NEUTRAL&quot;</td></tr><tr><td>&quot;RT @SmartReversals: $SPX - Dai…</td><td>&quot;NEUTRAL&quot;</td></tr><tr><td>&quot;$GOLD&#x27;s Massive Range.\n",
       "\n",
       "In the…</td><td>&quot;NEUTRAL&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 2)\n",
       "┌─────────────────────────────────┬───────────┐\n",
       "│ description                     ┆ sentiment │\n",
       "│ ---                             ┆ ---       │\n",
       "│ str                             ┆ str       │\n",
       "╞═════════════════════════════════╪═══════════╡\n",
       "│ Currently at $2860--Documentin… ┆ NEUTRAL   │\n",
       "│ $NET                            ┆ NEUTRAL   │\n",
       "│                                 ┆           │\n",
       "│ Was waiting on Cloudfla…        ┆           │\n",
       "│ RT @TrendSpider: $SPY, $QQQ an… ┆ NEUTRAL   │\n",
       "│ RT @TriggerTrades: $SPX remain… ┆ NEUTRAL   │\n",
       "│ RT @coiledspringcap: #SPX equa… ┆ NEUTRAL   │\n",
       "│ RT @coiledspringcap: Everyone … ┆ NEUTRAL   │\n",
       "│ RT @SmartReversals: $IWM - Dai… ┆ NEUTRAL   │\n",
       "│ RT @SmartReversals: $NDX - Dai… ┆ NEUTRAL   │\n",
       "│ RT @SmartReversals: $SPX - Dai… ┆ NEUTRAL   │\n",
       "│ $GOLD's Massive Range.          ┆ NEUTRAL   │\n",
       "│                                 ┆           │\n",
       "│ In the…                         ┆           │\n",
       "└─────────────────────────────────┴───────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Process tweets using the resumed concurrent approach\n",
    "tweets = sample_df[tweet_column].to_list()\n",
    "\n",
    "# Define parameters\n",
    "BATCH_SIZE = 4  # Number of tweets to process in each batch\n",
    "MAX_WORKERS = 2  # Number of concurrent workers (match to number of API keys)\n",
    "SAVE_EVERY = 100  # Save results every N tweets processed\n",
    "RATE_LIMIT_WAIT = 0  # Default wait time in seconds when rate limited\n",
    "\n",
    "# Change this to the batch you want to resume from\n",
    "resume_batch = int(6000 / BATCH_SIZE)\n",
    "\n",
    "# Process tweets using concurrent API keys with resuming capability\n",
    "sentiments = process_tweets_concurrent_resumed(\n",
    "    tweets, \n",
    "    start_batch=resume_batch,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    max_workers=MAX_WORKERS,\n",
    "    save_every=SAVE_EVERY,\n",
    "    rate_limit_wait=RATE_LIMIT_WAIT,\n",
    "    force_reprocess=True\n",
    ")\n",
    "\n",
    "# Add sentiments to the DataFrame\n",
    "sample_df = sample_df.with_columns(pl.Series(name='sentiment', values=sentiments))\n",
    "\n",
    "# Display the results\n",
    "sample_df.select([tweet_column, 'sentiment']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved labeled data to ../data/labeled_stock_tweets.csv\n",
      "\n",
      "Examples for each sentiment:\n",
      "\n",
      "STRONGLY_POSITIVE:\n",
      "$AMZN NEW ATH TRADES $200+\n",
      "\n",
      "POSITIVE:\n",
      "$GWAV Shorts worked this one back down. New support level at $4, watching for another pop back towards $4.60/$4.85+\n",
      "\n",
      "> [@Bullish_Trades](https://twitter.com/Bullish_Trades):\n",
      "> $GWAV $5.30+ hit, solid trade\n",
      "\n",
      "NEUTRAL:\n",
      "Scalp longed #BTC at $61.5k, new entry. Comment your setup.\n",
      "\n",
      "NEGATIVE:\n",
      "Meme stonks running hard after a potential htf top is put in on the indexes? \n",
      "\n",
      "Not usually a good sign \n",
      "\n",
      "$amc $gme $bbby\n",
      "\n",
      "STRONGLY_NEGATIVE:\n",
      "Right Click - > Inspect - > 🪄 \n",
      "\n",
      "Future Millionaires\n",
      "\n",
      "> [@IndicatorsWork](https://twitter.com/IndicatorsWork):\n",
      "> Let’s EXPOSE all the Robinhood Traders on X 🤡 #dontgetscammed\n"
     ]
    }
   ],
   "source": [
    "# Save the labeled data\n",
    "output_path = \"../data/labeled_stock_tweets.csv\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "sample_df.write_csv(output_path)\n",
    "print(f\"\\nSaved labeled data to {output_path}\")\n",
    "\n",
    "# Display some examples of each sentiment\n",
    "print(\"\\nExamples for each sentiment:\")\n",
    "for sentiment in ['STRONGLY_POSITIVE', 'POSITIVE', 'NEUTRAL', 'NEGATIVE', 'STRONGLY_NEGATIVE']:\n",
    "    examples = sample_df.filter(pl.col(\"sentiment\") == sentiment).sample(1, seed=42)\n",
    "    if examples.shape[0] > 0:\n",
    "        print(f\"\\n{sentiment}:\\n{examples[0, tweet_column]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
