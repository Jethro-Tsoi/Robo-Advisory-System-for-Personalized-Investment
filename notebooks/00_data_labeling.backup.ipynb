{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (1.26.0)\n",
      "Requirement already satisfied: requests in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jethrotsoi/anaconda3/envs/PIRAS/lib/python3.11/site-packages (from requests) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install polars requests tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d595140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "# Removed: # Removed: import google.generativeai as genai\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e855cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyManager:\n",
    "    \"\"\"Manages multiple API keys and rotates them when rate limits are reached\"\"\"\n",
    "    \n",
    "    def __init__(self, env_prefix='MISTRAL_API_KEY'):\n",
    "        \"\"\"\n",
    "        Initialize the key manager\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        env_prefix : str\n",
    "            Prefix for environment variables storing API keys.\n",
    "            Keys should be named like MISTRAL_API_KEY, MISTRAL_API_KEY_1, MISTRAL_API_KEY_2, etc.\n",
    "        \"\"\"\n",
    "        self.env_prefix = env_prefix\n",
    "        self.api_keys = self._load_api_keys()\n",
    "        self.current_index = 0\n",
    "        self.rate_limited_keys = {}  # Track which keys hit rate limits and when they can be used again\n",
    "        self.base_url = \"https://api.mistral.ai/v1/chat/completions\"\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        if not self.api_keys:\n",
    "            print(\"\u26a0\ufe0f No API keys found. Please set at least one API key.\")\n",
    "            key = input(f\"Enter your Mistral AI API key: \")\n",
    "            if key:\n",
    "                self.api_keys.append(key)\n",
    "            else:\n",
    "                raise ValueError(\"No API key provided. Cannot continue.\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.api_keys)} Mistral API keys.\")\n",
    "        \n",
    "    def _load_api_keys(self):\n",
    "        \"\"\"Load API keys from environment variables\"\"\"\n",
    "        api_keys = []\n",
    "        \n",
    "        # Try the base key first\n",
    "        base_key = os.getenv(self.env_prefix)\n",
    "        if base_key:\n",
    "            api_keys.append(base_key)\n",
    "        \n",
    "        # Try numbered keys (MISTRAL_API_KEY_1, MISTRAL_API_KEY_2, etc.)\n",
    "        for i in range(1, 10):  # Check for up to 10 keys\n",
    "            key = os.getenv(f\"{self.env_prefix}_{i}\")\n",
    "            if key:\n",
    "                api_keys.append(key)\n",
    "        \n",
    "        return api_keys\n",
    "    \n",
    "    def get_current_key(self):\n",
    "        \"\"\"Get the current active API key\"\"\"\n",
    "        return self.api_keys[self.current_index]\n",
    "    \n",
    "    def get_current_headers(self):\n",
    "        \"\"\"Get headers with the current API key\"\"\"\n",
    "        headers = self.headers.copy()\n",
    "        headers[\"Authorization\"] = f\"Bearer {self.get_current_key()}\"\n",
    "        return headers\n",
    "    \n",
    "    def rotate_key(self, rate_limited=False, retry_after=60):\n",
    "        \"\"\"\n",
    "        Rotate to the next available API key\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        rate_limited : bool\n",
    "            Whether the current key hit a rate limit\n",
    "        retry_after : int\n",
    "            Seconds until the rate-limited key can be used again\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str : The new API key\n",
    "        \"\"\"\n",
    "        # Mark the current key as rate limited if needed\n",
    "        if rate_limited:\n",
    "            self.rate_limited_keys[self.current_index] = time.time() + retry_after\n",
    "            print(f\"API key {self.current_index + 1} rate limited. Will retry after {retry_after} seconds.\")\n",
    "        \n",
    "        # Try to find a key that's not rate limited\n",
    "        original_index = self.current_index\n",
    "        while True:\n",
    "            self.current_index = (self.current_index + 1) % len(self.api_keys)\n",
    "            \n",
    "            # Check if this key is rate limited\n",
    "            if self.current_index in self.rate_limited_keys:\n",
    "                # Check if enough time has passed\n",
    "                if time.time() > self.rate_limited_keys[self.current_index]:\n",
    "                    # Key is no longer rate limited\n",
    "                    del self.rate_limited_keys[self.current_index]\n",
    "                    break\n",
    "            else:\n",
    "                # Key is not rate limited\n",
    "                break\n",
    "                \n",
    "            # If we've checked all keys and they're all rate limited, use the least recently rate limited one\n",
    "            if self.current_index == original_index:\n",
    "                # Find the key that will be available soonest\n",
    "                soonest_available = min(self.rate_limited_keys.items(), key=lambda x: x[1])\n",
    "                self.current_index = soonest_available[0]\n",
    "                wait_time = max(0, self.rate_limited_keys[self.current_index] - time.time())\n",
    "                \n",
    "                if wait_time > 0:\n",
    "                    print(f\"All API keys are rate limited. Waiting {wait_time:.1f} seconds for the next available key.\")\n",
    "                    time.sleep(wait_time)\n",
    "                    del self.rate_limited_keys[self.current_index]\n",
    "                break\n",
    "        \n",
    "        # Return the new key\n",
    "        key = self.api_keys[self.current_index]\n",
    "        print(f\"Switched to Mistral API key {self.current_index + 1}\")\n",
    "        return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e42b84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 Mistral API keys.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Key Manager\n",
    "key_manager = KeyManager()\n",
    "\n",
    "# Set the model name\n",
    "MODEL = \"mistral-small-latest\"  # Using Mistral's large model instead of Gemini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c9bc61",
   "metadata": {},
   "source": [
    "\n",
    "> **\u26a0\ufe0f API Key Setup**\n",
    ">\n",
    "> This notebook uses the Mistral AI API with support for multiple API keys:\n",
    ">\n",
    "> 1. Set your primary API key as `MISTRAL_API_KEY` environment variable\n",
    "> 2. For additional keys, use `MISTRAL_API_KEY_1`, `MISTRAL_API_KEY_2`, etc.\n",
    "> 3. The system will automatically rotate between keys if rate limits are encountered\n",
    ">\n",
    "> Keys can be created at [Mistral AI Platform](https://console.mistral.ai/)\n",
    ">\n",
    "> **Mistral AI Free Tier Limits:**\n",
    "> - 1 request per second (60 requests per minute)\n",
    "> - 500,000 tokens per minute\n",
    "> - 1 billion tokens per month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "700448bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_prompt(text):\n",
    "    \"\"\"Configure the prompt for Mistral\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "            You are a financial sentiment analyzer. Classify the given tweet's sentiment into one of these categories:\n",
    "\n",
    "            STRONGLY_POSITIVE - Very bullish, highly confident optimistic outlook\n",
    "            POSITIVE - Generally optimistic, bullish view\n",
    "            NEUTRAL - Factual, balanced, or no clear sentiment\n",
    "            NEGATIVE - Generally pessimistic, bearish view\n",
    "            STRONGLY_NEGATIVE - Very bearish, highly confident pessimistic outlook\n",
    "\n",
    "            Examples:\n",
    "            \"Breaking: Company XYZ doubles profit forecast!\" -> STRONGLY_POSITIVE\n",
    "            \"Expecting modest gains next quarter\" -> POSITIVE\n",
    "            \"Market closed at 35,000\" -> NEUTRAL\n",
    "            \"Concerned about rising rates\" -> NEGATIVE\n",
    "            \"Crash incoming, sell everything!\" -> STRONGLY_NEGATIVE\n",
    "\n",
    "            Format: Return only one word from: STRONGLY_POSITIVE, POSITIVE, NEUTRAL, NEGATIVE, STRONGLY_NEGATIVE\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Analyze the sentiment of this tweet: {text}\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbf82b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text, retries=3):\n",
    "    \"\"\"Get sentiment from Gemini with retry logic\"\"\"\n",
    "    if not text or len(str(text).strip()) < 3:\n",
    "        return 'NEUTRAL'\n",
    "    \n",
    "    # Create a generative model object\n",
    "    model = genai.GenerativeModel(MODEL, generation_config={\n",
    "        \"temperature\": 0.0,  # Deterministic output\n",
    "        \"max_output_tokens\": 10,  # We only need one word\n",
    "        \"top_p\": 1.0\n",
    "    })\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # Generate content with the model\n",
    "            response = model.generate_content(setup_prompt(text))\n",
    "            \n",
    "            # Extract the sentiment from the response\n",
    "            if hasattr(response, 'text'):\n",
    "                sentiment = response.text.strip().upper()\n",
    "                \n",
    "                # Validate the response\n",
    "                valid_labels = [\n",
    "                    'STRONGLY_POSITIVE', 'POSITIVE', 'NEUTRAL', 'NEGATIVE', 'STRONGLY_NEGATIVE'\n",
    "                ]\n",
    "                \n",
    "                if sentiment in valid_labels:\n",
    "                    return sentiment\n",
    "                else:\n",
    "                    print(f\"Invalid sentiment received: {sentiment}, defaulting to NEUTRAL\")\n",
    "                    return 'NEUTRAL'\n",
    "            else:\n",
    "                print(f\"Unexpected response format: {response}\")\n",
    "                if attempt < retries - 1:\n",
    "                    time.sleep(2)  # Wait before retry\n",
    "                    continue\n",
    "                else:\n",
    "                    return 'NEUTRAL'\n",
    "                    \n",
    "        except Exception as e:\n",
    "            error_str = str(e).lower()\n",
    "            # Check for rate limiting errors\n",
    "            if \"quota\" in error_str or \"rate\" in error_str or \"429\" in error_str:\n",
    "                # Extract retry time if available (default to 60 seconds if not found)\n",
    "                retry_after = 60\n",
    "                if \"retryafter\" in error_str or \"retry-after\" in error_str or \"retry_after\" in error_str:\n",
    "                    try:\n",
    "                        # Try to extract the retry time\n",
    "                        import re\n",
    "                        matches = re.findall(r'retry.*?(\\d+)', error_str)\n",
    "                        if matches:\n",
    "                            retry_after = int(matches[0])\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                # Switch to another API key if there are multiple keys\n",
    "                if len(key_manager.api_keys) > 1:\n",
    "                    key_manager.rotate_key(rate_limited=True, retry_after=retry_after)\n",
    "                    if attempt < retries - 1:\n",
    "                        continue\n",
    "                else:\n",
    "                    # Only one key, just wait\n",
    "                    wait_time = min(2 ** attempt * 5, retry_after)  # Exponential backoff with max retry_after\n",
    "                    print(f\"Rate limit hit - waiting {wait_time}s before retry ({attempt+1}/{retries})\")\n",
    "                    time.sleep(wait_time)\n",
    "                    if attempt < retries - 1:\n",
    "                        continue\n",
    "                    \n",
    "            if attempt == retries - 1:\n",
    "                print(f\"Error processing text: {str(text)[:50]}...\\nError: {str(e)}\")\n",
    "                return 'NEUTRAL'\n",
    "            time.sleep(2)  # Wait before retry\n",
    "    \n",
    "    return 'NEUTRAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7cc0b913",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'genai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test the sentiment analysis with key rotation\u001b[39;00m\n\u001b[32m      2\u001b[39m test_tweet = \u001b[33m\"\u001b[39m\u001b[33mBreaking: Tesla stock hits all-time high after unexpected profit surge\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m sentiment = \u001b[43mget_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_tweet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest tweet: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_tweet\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSentiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentiment\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mget_sentiment\u001b[39m\u001b[34m(text, retries)\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mNEUTRAL\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Create a generative model object\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model = \u001b[43mgenai\u001b[49m.GenerativeModel(MODEL, generation_config={\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.0\u001b[39m,  \u001b[38;5;66;03m# Deterministic output\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m5\u001b[39m,  \u001b[38;5;66;03m# We only need one word\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1.0\u001b[39m\n\u001b[32m     11\u001b[39m })\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries):\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     15\u001b[39m         \u001b[38;5;66;03m# Generate content with the model\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'genai' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the sentiment analysis with key rotation\n",
    "test_tweet = \"Breaking: Tesla stock hits all-time high after unexpected profit surge\"\n",
    "sentiment = get_sentiment(test_tweet)\n",
    "print(f\"Test tweet: '{test_tweet}'\")\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Using Mistral API key index: {key_manager.current_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Hugging Face\n",
    "print(\"Loading stock market tweets dataset from Hugging Face...\")\n",
    "\n",
    "# Check if the huggingface datasets library is installed\n",
    "try:\n",
    "    import huggingface_hub\n",
    "except ImportError:\n",
    "    print(\"Installing huggingface_hub...\")\n",
    "    !pip install huggingface_hub\n",
    "    import huggingface_hub\n",
    "\n",
    "# Load the dataset using Polars\n",
    "df = pl.read_csv('hf://datasets/mjw/stock_market_tweets/stock_market_tweets.csv')\n",
    "\n",
    "print(f\"Loaded {df.shape[0]} tweets\")\n",
    "print(\"\\nSample tweets:\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for sentiment analysis\n",
    "# Let's make sure we have the 'body' column which contains the tweet text\n",
    "if 'body' in df.columns:\n",
    "    tweet_column = 'body'\n",
    "elif 'full_text' in df.columns:\n",
    "    tweet_column = 'full_text'\n",
    "else:\n",
    "    raise ValueError(\"Could not find tweet text column in the dataset\")\n",
    "\n",
    "print(f\"Using '{tweet_column}' column for tweet text\")\n",
    "\n",
    "# For demonstration, let's use a small subset of the data\n",
    "# Use all data instead of a sample - WARNING: This will process 1.7M tweets!\n",
    "sample_size = df.shape[0]  # Adjust based on your needs\n",
    "sample_df = df.sample(sample_size, seed=42)\n",
    "\n",
    "print(f\"\\nAnalyzing sentiment for {sample_size} tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb1756",
   "metadata": {},
   "source": [
    "\n",
    "## Rate Limits for Mistral AI API\n",
    "\n",
    "Mistral AI's free tier has the following limits:\n",
    "- 1 request per second (60 requests per minute)\n",
    "- 500,000 tokens per minute\n",
    "- 1 billion tokens per month\n",
    "\n",
    "This notebook implements:\n",
    "1. Key rotation to handle multiple API keys\n",
    "2. Automatic retry with exponential backoff\n",
    "3. Batch processing to optimize throughput\n",
    "4. Error handling to ensure robust processing\n",
    "\n",
    "If you need to process many tweets, consider:\n",
    "- Creating multiple API keys\n",
    "- Adjusting batch size and workers based on your needs\n",
    "- Processing tweets in smaller batches with appropriate delays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweets, batch_size=4, max_workers=2):\n",
    "    \"\"\"Process tweets in batches with Google Gemini API\n",
    "    \n",
    "    Google Gemini API free tier allows:\n",
    "    - Up to 1,500 requests per day\n",
    "    - Higher throughput than OpenRouter's free tier\n",
    "    \"\"\"\n",
    "    all_sentiments = []\n",
    "    \n",
    "    # For demonstration, we'll process a reasonable number of tweets\n",
    "    # Adjust max_tweets if needed - 200 is safe for the free tier\n",
    "    max_tweets = min(200, len(tweets))\n",
    "    tweets = tweets[:max_tweets]\n",
    "    \n",
    "    print(f\"Processing {max_tweets} tweets using Google's Gemini API with {len(key_manager.api_keys)} API keys\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for i in tqdm(range(0, len(tweets), batch_size), desc=\"Processing tweet batches\"):\n",
    "            batch = tweets[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                # Process tweets in parallel\n",
    "                results = list(executor.map(get_sentiment, batch))\n",
    "                all_sentiments.extend(results)\n",
    "                \n",
    "                # Add a short delay between batches to avoid rate limiting\n",
    "                time.sleep(2)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "                # Add neutral sentiments for this batch in case of failure\n",
    "                all_sentiments.extend(['NEUTRAL'] * len(batch))\n",
    "                time.sleep(5)  # Wait a bit longer after an error\n",
    "                \n",
    "    # If we didn't get enough sentiments (due to errors), fill with NEUTRAL\n",
    "    if len(all_sentiments) < len(tweets):\n",
    "        all_sentiments.extend(['NEUTRAL'] * (len(tweets) - len(all_sentiments)))\n",
    "            \n",
    "    return all_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef2b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test the sentiment analysis with key rotation\n",
    "test_tweet = \"Breaking: Tesla stock hits all-time high after unexpected profit surge\"\n",
    "sentiment = get_sentiment(test_tweet)\n",
    "print(f\"Test tweet: '{test_tweet}'\")\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Using API key index: {key_manager.current_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b23691ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweets, batch_size=4, max_workers=2):\n",
    "    \"\"\"Process tweets in batches with Mistral AI API\n",
    "    \n",
    "    Mistral AI free tier allows:\n",
    "    - 1 request per second (60 requests per minute)\n",
    "    - 500,000 tokens per minute\n",
    "    - 1 billion tokens per month\n",
    "    \"\"\"\n",
    "    all_sentiments = []\n",
    "    \n",
    "    # Process all tweets\n",
    "    # Removed limitation: max_tweets = min(200, len(tweets))\n",
    "    # Removed limitation: tweets = tweets[:max_tweets]\n",
    "    \n",
    "    print(f\"Processing {len(tweets)} tweets using Mistral AI API with {len(key_manager.api_keys)} API keys\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for i in tqdm(range(0, len(tweets), batch_size), desc=\"Processing tweet batches\"):\n",
    "            batch = tweets[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                # Process tweets in parallel\n",
    "                results = list(executor.map(get_sentiment, batch))\n",
    "                all_sentiments.extend(results)\n",
    "                \n",
    "                # Add a delay between batches to respect Mistral's 1 req/sec rate limit\n",
    "                # With batch_size and max_workers, adjust sleep time accordingly\n",
    "                time.sleep(batch_size * max_workers)  # Conservative approach\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {e}\")\n",
    "                # Add neutral sentiments for this batch in case of failure\n",
    "                all_sentiments.extend(['NEUTRAL'] * len(batch))\n",
    "                time.sleep(5)  # Wait a bit longer after an error\n",
    "                \n",
    "    # If we didn't get enough sentiments (due to errors), fill with NEUTRAL\n",
    "    if len(all_sentiments) < len(tweets):\n",
    "        all_sentiments.extend(['NEUTRAL'] * (len(tweets) - len(all_sentiments)))\n",
    "            \n",
    "    return all_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10544ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a subset of tweets\n",
    "tweets = sample_df[tweet_column].to_list()\n",
    "sentiments = process_tweets(tweets, batch_size=4, max_workers=8)\n",
    "\n",
    "# Add sentiments to the DataFrame\n",
    "sample_df = sample_df.with_columns(pl.Series(name='sentiment', values=sentiments))\n",
    "\n",
    "# Display the results\n",
    "sample_df.select([tweet_column, 'sentiment']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the labeled data\n",
    "output_path = \"../data/labeled_stock_tweets.csv\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "sample_df.write_csv(output_path)\n",
    "print(f\"\\nSaved labeled data to {output_path}\")\n",
    "\n",
    "# Display some examples of each sentiment\n",
    "print(\"\\nExamples for each sentiment:\")\n",
    "for sentiment in ['STRONGLY_POSITIVE', 'POSITIVE', 'NEUTRAL', 'NEGATIVE', 'STRONGLY_NEGATIVE']:\n",
    "    examples = sample_df.filter(pl.col(\"sentiment\") == sentiment).sample(1, seed=42)\n",
    "    if examples.shape[0] > 0:\n",
    "        print(f\"\\n{sentiment}:\\n{examples[0, tweet_column]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}